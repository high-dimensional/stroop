{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph lesion deficit mapping model of stroop\n",
    "# J Ruffle j.ruffle@ucl.ac.uk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph_tool.all import *; import graph_tool.all as gt; import  matplotlib; import math; import numpy; import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import matplotlib\n",
    "#matplotlib.use('Agg')\n",
    "import pandas as pd\n",
    "import scipy as scipy\n",
    "import seaborn as sns\n",
    "from pylab import *\n",
    "from multiprocessing import  Pool\n",
    "import subprocess\n",
    "import scipy.sparse as sparse\n",
    "from scipy.io import savemat\n",
    "# sns.set_theme()\n",
    "import pickle\n",
    "pd.set_option('display.max_columns', 500)\n",
    "import glob\n",
    "\n",
    "import nibabel as nib\n",
    "\n",
    "from nilearn import plotting\n",
    "from nilearn import surface\n",
    "from nilearn import datasets\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "from tqdm.notebook import tqdm \n",
    "\n",
    "custom_params = {\"axes.spines.right\": False, \"axes.spines.top\": False}\n",
    "sns.set_theme(style=\"ticks\", rc=custom_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "gt.seed_rng(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_options = ['analogical_reasoning','rapm','deductive_reasoning','hayling','proverbs','fluency','stroop']\n",
    "print(task_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug=False\n",
    "print('Debug mode: '+str(debug))\n",
    "\n",
    "task = task_options[6]\n",
    "print(\"Task here will be...\"+str(task))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/media/Data1/NewIQ/jruffle_analysis/mni_masks_444/network_files/'\n",
    "out = '/media/Data1/NewIQ/jruffle_analysis/results/'\n",
    "subs = pd.read_csv('/media/Data1/NewIQ/jruffle_analysis/subs.txt',header=None)[0]\n",
    "print(len(subs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('/media/Data1/NewIQ/Analogical_Reasoning_31_03_23.xlsx')\n",
    "print(df.shape)\n",
    "print(df.columns)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Anallogical_Reasoning_Task_overall_accuracy_ (% correct)'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_excel('/media/Data1/NewIQ/Data.xlsx')\n",
    "print(df2.shape)\n",
    "print(df2.columns)\n",
    "# df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['Analogical_Reasoning_Task_accuracy (% correct)'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.read_excel('/media/Data1/NewIQ/RAPM_31_03_23.xlsx')\n",
    "print(df3.shape)\n",
    "print(df3.columns)\n",
    "# df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['RAPM'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = pd.read_excel('/media/Data1/NewIQ/Deductive_reasoning_31_03_23.xlsx')\n",
    "print(df4.shape)\n",
    "print(df4.columns)\n",
    "# df4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4['Deductive_Reasoning_Task_overall_accuracy (% correct)'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = pd.read_excel('/media/Data1/NewIQ/Additionals.xlsx')\n",
    "print(df5.shape)\n",
    "print(df5.columns)\n",
    "# df5.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hayling = pd.read_excel('/media/Data1/NewIQ/Hayling_3_05_23.xlsx')\n",
    "# hayling.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hayling['Hayling_overall_scaled_score'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proverbs = pd.read_excel('/media/Data1/NewIQ/Proverbs_3_05_23.xlsx')\n",
    "# proverbs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proverbs['Proverbs_total_score'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fluency = pd.read_excel('/media/Data1/NewIQ/S_Fluency_3_05_23.xlsx')\n",
    "# fluency.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fluency['S_fluency_correct_words'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stroop = pd.read_excel('/media/Data1/NewIQ/Stroop_3_05_23.xlsx')\n",
    "# stroop.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stroop['Stroop_words_correct'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if task=='analogical_reasoning':\n",
    "    df = df\n",
    "    target = 'Anallogical_Reasoning_Task_overall_accuracy_ (% correct)'\n",
    "\n",
    "if task=='rapm':\n",
    "    df = df3\n",
    "    target = 'RAPM'\n",
    "    \n",
    "if task=='deductive_reasoning':\n",
    "    df = df4\n",
    "    target = 'Deductive_Reasoning_Task_overall_accuracy (% correct)'\n",
    "    \n",
    "if task=='hayling':\n",
    "    df = hayling\n",
    "    target = 'Hayling_overall_scaled_score'\n",
    "    \n",
    "if  task=='proverbs':\n",
    "    df = proverbs\n",
    "    target = 'Proverbs_total_score'\n",
    "    \n",
    "if task=='fluency':\n",
    "    df = fluency\n",
    "    target = 'S_fluency_correct_words'\n",
    "    \n",
    "if task == 'stroop':\n",
    "    df = stroop\n",
    "    target = 'Stroop_words_correct'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Hosp_Number']=''\n",
    "for i, row in df.iterrows():\n",
    "    df.loc[i,'Hosp_Number']=row['Original_scan_filename'].split('_')[0].split('.')[0]\n",
    "\n",
    "out=out+task+'/'\n",
    "    \n",
    "isExist = os.path.exists(out)\n",
    "if not isExist:\n",
    "\n",
    "   # Create a new directory because it does not exist\n",
    "   os.makedirs(out)\n",
    "   print(\"The new directory is created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs = df['Hosp_Number']\n",
    "\n",
    "print(len(subs))\n",
    "subs\n",
    "\n",
    "template_nodes = pd.read_csv(path+'MNI152_T1_nodes.txt',delim_whitespace=True)\n",
    "template_nodes['coords9dig']=template_nodes['coords9dig'].apply(lambda x: '{0:0>9}'.format(x))\n",
    "template_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import datasets, image\n",
    "niimg = nib.load('/media/Data1/NewIQ/jruffle_analysis/mni_masks_444/MNI152_T1_mni444_thresh.nii.gz')\n",
    "\n",
    "def parallelize_dataframe(df, func, n_cores=40):\n",
    "    df_split = np.array_split(df, n_cores)\n",
    "    pool = Pool(n_cores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n",
    "\n",
    "def add_features(df):\n",
    "    df['mni_x'], df['mni_y'], df['mni_z'] = image.coord_transform(df['x'], df['y'], df['z'], niimg.affine)\n",
    "    bashCommand = 'atlasquery -a \"Talairach Daemon Labels\" -c '+str(np.array(df['mni_x'])[0])+','+str(np.array(df['mni_y'])[0])+','+str(np.array(df['mni_z'])[0]) ##prepare the subprocess for FSL atlasquery\n",
    "    region = subprocess.check_output(['bash','-c',bashCommand])\n",
    "    \n",
    "    bashCommand = 'atlasquery -a \"Harvard-Oxford Cortical Structural Atlas\" -c '+str(np.array(df['mni_x'])[0])+','+str(np.array(df['mni_y'])[0])+','+str(np.array(df['mni_z'])[0]) ##prepare the subprocess for FSL atlasquery\n",
    "    region_harvard_oxford = subprocess.check_output(['bash','-c',bashCommand])\n",
    "                                                                                                          \n",
    "    \n",
    "    df['mni_x']=df['mni_x'].astype(str)\n",
    "    df['mni_y']=df['mni_y'].astype(str)\n",
    "    df['mni_z']=df['mni_z'].astype(str)\n",
    "    \n",
    "#     df['label0']=region.decode().split('.')[0].rstrip(\"\\n\") ##execute it\n",
    "    df['label1']=region.decode().split('.')[1].rstrip(\"\\n\") ##execute it\n",
    "    df['label2']=region.decode().split('.')[2].rstrip(\"\\n\") ##execute it\n",
    "    df['label3']=region.decode().split('.')[3].rstrip(\"\\n\") ##execute it\n",
    "#     df['label4']=region.decode().split('.')[4].rstrip(\"\\n\") ##execute it\n",
    "\n",
    "    df['harvard_oxford_cortical']=region_harvard_oxford.decode().split(',')[0].rstrip(\"\\n\").strip().replace('<b>Harvard-Oxford Cortical Structural Atlas</b><br>','')\n",
    "    \n",
    "    return df\n",
    "\n",
    "template_nodes = parallelize_dataframe(template_nodes, add_features)\n",
    "\n",
    "template_nodes['MNI coordinates']=1\n",
    "for i,row in template_nodes.iterrows():\n",
    "    template_nodes.iloc[i,template_nodes.shape[1]-1]=str(template_nodes.iloc[i,16])+' '+str(template_nodes.iloc[i,17])+' '+str(template_nodes.iloc[i,18])\n",
    "    \n",
    "template_nodes['label3_int']=template_nodes['label3']\n",
    "template_nodes['label3_int'].replace('*',0,inplace=True)\n",
    "template_nodes['label3_int'].replace('White Matter',1,inplace=True)\n",
    "template_nodes['label3_int'].replace('Gray Matter',2,inplace=True)\n",
    "template_nodes['label3_int'].replace('Cerebro-Spinal Fluid',3,inplace=True)\n",
    "\n",
    "# template_nodes['label3_int'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Graph(directed=False)\n",
    "\n",
    "v_prop_coords = g.new_vertex_property(\"string\")\n",
    "v_prop_coords9dig = g.new_vertex_property(\"string\")\n",
    "v_prop_name_1 = g.new_vertex_property(\"string\")\n",
    "v_prop_name_2 = g.new_vertex_property(\"string\")\n",
    "v_prop_name_3 = g.new_vertex_property(\"string\")\n",
    "v_prop_name_mni = g.new_vertex_property(\"string\")\n",
    "v_prop_name_mni_x = g.new_vertex_property(\"string\")\n",
    "v_prop_name_mni_y = g.new_vertex_property(\"string\")\n",
    "v_prop_name_mni_z = g.new_vertex_property(\"string\")\n",
    "v_prop_tissue = g.new_vertex_property(\"int\")\n",
    "v_prop_harvard_label = g.new_vertex_property(\"string\")\n",
    "\n",
    "for vertexnumber, row in template_nodes.iterrows():\n",
    "    #node=\"v{}\".format(vertexnumber)\n",
    "    node = g.add_vertex()\n",
    "    \n",
    "    coords = (row['coords'])\n",
    "    coords9dig=(row['coords9dig'])\n",
    "    lab1 = (row['label1'])\n",
    "    lab2=(row['label2'])\n",
    "    lab3 = (row['label3'])\n",
    "    mni=(row['MNI coordinates'])\n",
    "    mni_x=(row['mni_x'])\n",
    "    mni_y=(row['mni_y'])\n",
    "    mni_z=(row['mni_z'])\n",
    "    tissue=(row['label3_int'])\n",
    "    harvard=(row['harvard_oxford_cortical'])\n",
    "\n",
    "    v_prop_coords[vertexnumber] = coords\n",
    "    v_prop_coords9dig[vertexnumber] = coords9dig\n",
    "    v_prop_name_1[vertexnumber] = lab1\n",
    "    v_prop_name_2[vertexnumber] = lab2\n",
    "    v_prop_name_3[vertexnumber] = lab3\n",
    "    v_prop_name_mni[vertexnumber] = mni\n",
    "    v_prop_name_mni_x[vertexnumber] = mni_x\n",
    "    v_prop_name_mni_y[vertexnumber] = mni_y\n",
    "    v_prop_name_mni_z[vertexnumber] = mni_z\n",
    "    v_prop_tissue[vertexnumber] = tissue\n",
    "    v_prop_harvard_label[vertexnumber]=harvard\n",
    "    \n",
    "\n",
    "g.vertex_properties[\"coords\"]=v_prop_coords\n",
    "g.vertex_properties[\"coords9dig\"]=v_prop_coords9dig\n",
    "g.vertex_properties[\"lab1\"]=v_prop_name_1\n",
    "g.vertex_properties[\"lab2\"]=v_prop_name_2\n",
    "g.vertex_properties[\"lab3\"]=v_prop_name_3\n",
    "g.vertex_properties[\"mni\"]=v_prop_name_mni\n",
    "g.vertex_properties[\"mni_x\"]=v_prop_name_mni_x\n",
    "g.vertex_properties[\"mni_y\"]=v_prop_name_mni_y\n",
    "g.vertex_properties[\"mni_z\"]=v_prop_name_mni_z\n",
    "g.vertex_properties[\"tissue\"]=v_prop_tissue\n",
    "g.vertex_properties[\"label_harvard_oxford\"]=v_prop_harvard_label\n",
    "\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_edges = pd.read_csv(path+'MNI152_T1_edges.txt',delim_whitespace=True)\n",
    "template_edges['CorNode1']=template_edges['CorNode1'].apply(lambda x: '{0:0>9}'.format(x))\n",
    "template_edges['CorNode2']=template_edges['CorNode2'].apply(lambda x: '{0:0>9}'.format(x))\n",
    "template_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUGGING\n",
    "if debug:\n",
    "    df = df[df[target]>0].reset_index(drop=True)\n",
    "    df = df.iloc[:5]\n",
    "    subs = df['Hosp_Number']\n",
    "    subs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nbs_adjacency_target_error = np.zeros(shape=(template_nodes.shape[0],template_nodes.shape[0]))\n",
    "nbs_adjacency_nart = np.zeros(shape=(template_nodes.shape[0],template_nodes.shape[0]))\n",
    "nbs_adjacency_weight = np.zeros(shape=(template_nodes.shape[0],template_nodes.shape[0]))\n",
    "\n",
    "nbs_adjacency_sol = np.zeros(shape=(template_nodes.shape[0],template_nodes.shape[0]))\n",
    "nbs_adjacency_ischaemia = np.zeros(shape=(template_nodes.shape[0],template_nodes.shape[0]))\n",
    "\n",
    "counter=0\n",
    "\n",
    "for j in tqdm(range(len(subs))):\n",
    "\n",
    "    sub = subs[j]\n",
    "#     print(\"Working on ... \"+str(sub))\n",
    "#     print(\"\")\n",
    "    \n",
    "    target_error = df.loc[j,target]\n",
    "\n",
    "    g.set_edge_filter(None)\n",
    "    g.set_vertex_filter(None)\n",
    "\n",
    "    node_file = glob.glob(path+'*'+sub+'*nodes.txt')\n",
    "    edge_file = glob.glob(path+'*'+sub+'*edges.txt')\n",
    "    \n",
    "    if len(node_file)>0 and len(edge_file)>0:\n",
    "        \n",
    "        sub_nodes = pd.read_csv(node_file[0],delim_whitespace=True)\n",
    "        sub_nodes['coords9dig']=sub_nodes['coords9dig'].apply(lambda x: '{0:0>9}'.format(x))\n",
    "\n",
    "        sub_edges = pd.read_csv(edge_file[0],delim_whitespace=True)\n",
    "        sub_edges['CorNode1']=sub_edges['CorNode1'].apply(lambda x: '{0:0>9}'.format(x))\n",
    "        sub_edges['CorNode2']=sub_edges['CorNode2'].apply(lambda x: '{0:0>9}'.format(x))\n",
    "\n",
    "\n",
    "#         print(\"Commencing edge compilation\")\n",
    "\n",
    "        arr = pd.DataFrame(sub_edges.iloc[:,:3])\n",
    "        arr = template_edges[template_edges.set_index(['CorNode1','CorNode2']).index.isin(arr.set_index(['CorNode1','CorNode2']).index)]\n",
    "\n",
    "        target_error_arr = arr.copy()\n",
    "\n",
    "        #code for binary target\n",
    "    #     if target_error == 0:\n",
    "    #         target_error_arr.iloc[:,2]=0\n",
    "    #     else:\n",
    "    #         target_error_arr.iloc[:,2]=1\n",
    "\n",
    "        target_er0ror_arr.iloc[:,2]=target_error_arr.iloc[:,2]/target_error\n",
    "\n",
    "        adjacency_sub_target_error = sparse.coo_matrix((target_error_arr.iloc[:, 2], (target_error_arr.iloc[:, 3], target_error_arr.iloc[:, 4])), shape=nbs_adjacency_target_error.shape\n",
    "                                ).todense()\n",
    "\n",
    "\n",
    "        adjacency_sub_weight = sparse.coo_matrix((arr.iloc[:, 2], (arr.iloc[:, 3], arr.iloc[:, 4])), shape=nbs_adjacency_weight.shape\n",
    "                                ).todense()\n",
    "\n",
    "\n",
    "        nbs_adjacency_target_error=nbs_adjacency_target_error+adjacency_sub_target_error\n",
    "\n",
    "        nbs_adjacency_weight=nbs_adjacency_weight+adjacency_sub_weight\n",
    "\n",
    "        counter+=1\n",
    "        \n",
    "print(\"\")\n",
    "print(\"Complete\")\n",
    "print(\"Number of lesion networks available and utilised: \"+str(counter))\n",
    "print(\"\")\n",
    "    \n",
    "print(\"normalising by lesion covariate\")\n",
    "\n",
    "with np.errstate(divide='ignore', invalid='ignore'):\n",
    "    nbs_adjacency_target_error[nbs_adjacency_target_error == np.inf] = 0\n",
    "    nbs_adjacency_target_error = np.nan_to_num(nbs_adjacency_target_error)\n",
    "    \n",
    "print(\"Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_list = np.nonzero(nbs_adjacency_target_error)\n",
    "print(len(edge_list[0]))\n",
    "\n",
    "edge_list_df = pd.DataFrame(edge_list[0])\n",
    "edge_list_df['1']=edge_list[1]\n",
    "\n",
    "x = np.array(nbs_adjacency_target_error[np.nonzero(nbs_adjacency_target_error)]).flatten()\n",
    "\n",
    "if not debug:\n",
    "\n",
    "    # Calculate the percentiles across the x and y dimension\n",
    "    perc01 = np.percentile(x, 0.5, axis=0, keepdims=True)\n",
    "    perc99 = np.percentile(x, 99.5, axis=0, keepdims=True)\n",
    "\n",
    "    # Clip array with different limits across the z dimension\n",
    "    normalized = np.clip(x, a_min=perc01, a_max=perc99)\n",
    "    normalized = (normalized-min(normalized))/(max(normalized)-min(normalized))\n",
    "\n",
    "if debug:\n",
    "    normalized = x\n",
    "    \n",
    "edge_list_df['target_errorweight']=normalized\n",
    "edge_list_df['lesionweight']=0\n",
    "edge_list_df['layer']=1\n",
    "print(edge_list_df.head())\n",
    "edge_list_df.shape\n",
    "\n",
    "##threshold to top 10% for debugging...\n",
    "threshold=75\n",
    "\n",
    "if not debug:\n",
    "    thresh=np.percentile(edge_list_df['target_errorweight'], threshold, axis=0, keepdims=True)[0]\n",
    "    edge_list_df = edge_list_df[edge_list_df['target_errorweight'] > thresh]  \n",
    "edge_list_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_list_lw = np.nonzero(nbs_adjacency_weight)\n",
    "print(len(edge_list_lw[0]))\n",
    "\n",
    "edge_list_df2 = pd.DataFrame(edge_list_lw[0])\n",
    "edge_list_df2['1']=edge_list_lw[1]\n",
    "\n",
    "x = np.array(nbs_adjacency_weight[np.nonzero(nbs_adjacency_weight)]).flatten()\n",
    "\n",
    "if not debug:\n",
    "\n",
    "    # Calculate the percentiles across the x and y dimension\n",
    "    perc01 = np.percentile(x, 0.5, axis=0, keepdims=True)\n",
    "    perc99 = np.percentile(x, 99.5, axis=0, keepdims=True)\n",
    "\n",
    "    # Clip array with different limits across the z dimension\n",
    "    normalized = np.clip(x, a_min=perc01, a_max=perc99)\n",
    "    normalized = (normalized-min(normalized))/(max(normalized)-min(normalized))\n",
    "    \n",
    "if debug:\n",
    "    normalized = x\n",
    "\n",
    "edge_list_df2['target_errorweight']=0\n",
    "edge_list_df2['lesionweight']=normalized\n",
    "# edge_list_df2['lesionweight']=np.array(nbs_adjacency_weight[np.nonzero(nbs_adjacency_weight)]).flatten()\n",
    "edge_list_df2['layer']=-1\n",
    "print(edge_list_df2.head())\n",
    "edge_list_df2.shape\n",
    "\n",
    "##threshold to top 10% for debugging...\n",
    "if not debug:\n",
    "    thresh=np.percentile(edge_list_df2['lesionweight'], threshold, axis=0, keepdims=True)[0]\n",
    "    edge_list_df2 = edge_list_df2[edge_list_df2['lesionweight'] > thresh]  \n",
    "edge_list_df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [edge_list_df, edge_list_df2]\n",
    "edge_list_final = pd.concat(frames)\n",
    "edge_list_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_list_final['random_layer']=np.random.randint(len(np.unique(edge_list_final.iloc[:,4])), size=edge_list_final.shape[0])\n",
    "edge_list_final\n",
    "sns.histplot(edge_list_final['random_layer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = np.isinf(edge_list_final).values.sum()\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=edge_list_final,x='layer',y='random_layer')\n",
    "edge_list_final_corr = edge_list_final.iloc[:,2:].corr()\n",
    "sns.heatmap(edge_list_final_corr,annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_list_final = np.array(edge_list_final)\n",
    "edge_list_final\n",
    "\n",
    "target_errorweight = g.new_ep(\"double\")\n",
    "lesionweight = g.new_ep(\"double\")\n",
    "elayer = g.new_ep(\"int\")\n",
    "nullelayer = g.new_ep(\"int\")\n",
    "g.add_edge_list(edge_list_final, eprops=[target_errorweight,lesionweight, elayer,nullelayer])\n",
    "\n",
    "g.ep[\"target_error\"] = target_errorweight\n",
    "g.ep[\"lw\"] = lesionweight\n",
    "g.ep[\"layer\"] = elayer\n",
    "g.ep[\"nulllayer\"] = nullelayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_filter = g.new_edge_property(\"bool\") \n",
    "for e in g.edges(): \n",
    "        if g.ep.layer[e] ==1:\n",
    "            edge_filter[e] = True \n",
    "        else: \n",
    "            edge_filter[e] = False \n",
    "g.edge_properties[\"target_errorfilter\"]=edge_filter\n",
    "\n",
    "edge_filter = g.new_edge_property(\"bool\") \n",
    "for e in g.edges(): \n",
    "        if g.ep.layer[e] <0:\n",
    "            edge_filter[e] = True \n",
    "        else: \n",
    "            edge_filter[e] = False \n",
    "g.edge_properties[\"lesionfilter\"]=edge_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_target_error = g.copy()\n",
    "g_target_error.set_edge_filter(g_target_error.ep.target_errorfilter)\n",
    "g_target_error.purge_vertices()\n",
    "g_target_error.purge_edges()\n",
    "print(g_target_error)\n",
    "tree = gt.min_spanning_tree(g_target_error,weights=g_target_error.ep.target_error)\n",
    "g_target_error.ep[\"minspanningtree\"] = tree\n",
    "\n",
    "weights = pd.DataFrame(np.array(g_target_error.ep.target_error.get_array()))\n",
    "weights.rename(columns={0:task},inplace=True)\n",
    "\n",
    "q75, q25 = np.percentile(weights, [75 ,25])\n",
    "\n",
    "plt.figure(figsize=(2, 5))\n",
    "# sns.histplot(weights['target_error Weight'],kde=True,log_scale=True,bins=50).set(ylim=(None,None))\n",
    "sns.histplot(weights[task])\n",
    "# plt.axvline(q25,c='red',lw=2,linestyle='dashed')\n",
    "plt.xticks(rotation=30,horizontalalignment=\"right\")\n",
    "plt.savefig(path+'target_error_edges.png', dpi=600,bbox_inches='tight')\n",
    "\n",
    "g_target_error.set_edge_filter(g_target_error.ep.minspanningtree)\n",
    "g_target_error.purge_vertices()\n",
    "g_target_error.purge_edges()\n",
    "print(g_target_error)\n",
    "degree_target_error = g_target_error.degree_property_map(\"total\")\n",
    "\n",
    "gt.graph_draw(g_target_error,\n",
    "                vcmap=matplotlib.cm.inferno,edge_gradient=[],\n",
    "             vertex_fill_color=gt.prop_to_size(degree_target_error),\n",
    "              vertex_size=gt.prop_to_size(degree_target_error),\n",
    "              output=out+task+'_error_minspantree.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = np.isinf(weights[task]).values.sum()\n",
    "print(\"It contains \" + str(count) + \" infinite values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_lw = g.copy()\n",
    "g_lw.set_edge_filter(g_lw.ep.lesionfilter)\n",
    "g_lw.purge_vertices()\n",
    "g_lw.purge_edges()\n",
    "print(g_lw)\n",
    "tree = gt.min_spanning_tree(g_lw,weights=g_lw.ep.lw)\n",
    "g_lw.ep[\"minspanningtree\"] = tree\n",
    "\n",
    "weights = pd.DataFrame(np.array(g_lw.ep.lw.get_array()))\n",
    "weights.rename(columns={0:'Lesion Weight'},inplace=True)\n",
    "\n",
    "q75, q25 = np.percentile(weights, [75 ,25])\n",
    "\n",
    "plt.figure(figsize=(2, 5))\n",
    "# sns.histplot(weights['target_error Weight'],kde=True,log_scale=True,bins=50).set(ylim=(None,None))\n",
    "sns.histplot(weights['Lesion Weight'])\n",
    "# plt.axvline(q25,c='red',lw=2,linestyle='dashed')\n",
    "plt.xticks(rotation=30,horizontalalignment=\"right\")\n",
    "plt.savefig(path+'lesion_edges.png', dpi=600,bbox_inches='tight')\n",
    "\n",
    "\n",
    "g_lw.set_edge_filter(g_lw.ep.minspanningtree)\n",
    "g_lw.purge_vertices()\n",
    "g_lw.purge_edges()\n",
    "print(g_lw)\n",
    "degree_lw = g_lw.degree_property_map(\"total\")\n",
    "\n",
    "gt.graph_draw(g_lw,\n",
    "                vcmap=matplotlib.cm.winter,edge_gradient=[],\n",
    "             vertex_fill_color=gt.prop_to_size(degree_lw),\n",
    "              vertex_size=gt.prop_to_size(degree_lw),\n",
    "              output=out+'lesionweight_minspantree.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.set_edge_filter(None)\n",
    "print(g)\n",
    "g.ep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.save(out+\"graph_file_4mm_space_layered_thresh.xml.gz\")\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g =load_graph(out+\"graph_file_4mm_space_layered_thresh.xml.gz\")\n",
    "# g.set_edge_filter(None)\n",
    "# g.set_vertex_filter(None)\n",
    "print(g)\n",
    "g.vp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.set_vertex_filter(None)\n",
    "print(g)\n",
    "deg = g.degree_property_map(deg='total')\n",
    "g.vp['deg']=deg\n",
    "deg_filter = g.new_vertex_property(\"bool\") \n",
    "for v in g.vertices():\n",
    "    if deg[v]>0:\n",
    "        deg_filter[v] = True\n",
    "    else:\n",
    "        deg_filter[v] = False\n",
    "g.vp['deg_filter'] = deg_filter\n",
    "g.set_vertex_filter(deg_filter)\n",
    "print(g)\n",
    "g.purge_vertices()\n",
    "g.purge_edges()\n",
    "g.set_vertex_filter(None)\n",
    "print(g)\n",
    "\n",
    "v_list = []\n",
    "for v in g.vertices():\n",
    "    v_list.append(g.vp['coords9dig'][v])\n",
    "to_remove = set(list(template_nodes['coords9dig'].values))-set(v_list)\n",
    "len(to_remove)\n",
    "template_nodes = template_nodes[template_nodes['coords9dig'].isin(v_list)].reset_index(drop=True)\n",
    "template_nodes = template_nodes.set_index('coords9dig')\n",
    "template_nodes = template_nodes.loc[v_list].reset_index()\n",
    "\n",
    "sns.histplot(g.degree_property_map(deg='total').get_array())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LAYERED MODEL\n",
    "\n",
    "state_norm = gt.minimize_nested_blockmodel_dl(g,state_args=dict(base_type=gt.LayeredBlockState,\n",
    "                                                         state_args=dict(ec=g.ep.layer,recs = [g.ep.target_error,g.ep.lw],rec_types=[\"real-normal\",\"discrete-poisson\"], layers=True,deg_corr=True)))\n",
    "state_norm.entropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LAYERED MODEL\n",
    "\n",
    "state_exp = gt.minimize_nested_blockmodel_dl(g,state_args=dict(base_type=gt.LayeredBlockState,\n",
    "                                                         state_args=dict(ec=g.ep.layer,recs = [g.ep.target_error,g.ep.lw],rec_types=[\"real-exponential\",\"discrete-poisson\"], layers=True,deg_corr=True)))\n",
    "state_exp.entropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LAYERED MODEL\n",
    "\n",
    "state_poi = gt.minimize_nested_blockmodel_dl(g,state_args=dict(base_type=gt.LayeredBlockState,\n",
    "                                                         state_args=dict(ec=g.ep.layer,recs = [g.ep.target_error,g.ep.lw],rec_types=[\"discrete-poisson\",\"discrete-poisson\"], layers=True,deg_corr=True)))\n",
    "state_poi.entropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if state_norm.entropy() < state_exp.entropy():\n",
    "    if state_norm.entropy() < state_poi.entropy():\n",
    "        print(\"Normal distribution is best, for resampling\")\n",
    "        state=state_norm\n",
    "        \n",
    "if state_exp.entropy() < state_norm.entropy():\n",
    "    if state_exp.entropy() < state_poi.entropy():\n",
    "        print(\"Exponential distribution is best, for resampling\")\n",
    "        state=state_exp\n",
    "        \n",
    "if state_poi.entropy() < state_norm.entropy():\n",
    "    if state_poi.entropy() < state_exp.entropy():\n",
    "        print(\"Poisson distribution is best, for resampling\")\n",
    "        state=state_poi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# history = gt.mcmc_anneal(state, beta_range=(1, 10), mcmc_equilibrate_args=dict(force_niter=10),history=True,verbose=True)\n",
    "# history = gt.mcmc_anneal(state, beta_range=(1, 10),history=True,verbose=True)\n",
    "\n",
    "if debug:\n",
    "    iterations=5\n",
    "if not debug:\n",
    "    iterations=100\n",
    "\n",
    "history = gt.mcmc_anneal(state, beta_range=(1, 10), mcmc_equilibrate_args=dict(force_niter=iterations),history=True,verbose=True)\n",
    "\n",
    "state.entropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "sns.lineplot(data=history[3],palette=['k'],legend=False)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Description length (nats)')\n",
    "plt.savefig(out+'MCMC_entropy.png', dpi=600,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "state_nullelayer = gt.NestedBlockState(g,base_type=gt.LayeredBlockState,\n",
    "                                                         state_args=dict(ec=g.ep.nulllayer,recs = [g.ep.target_error,g.ep.lw],rec_types=[\"real-normal\",\"discrete-poisson\"], layers=True,deg_corr=True))\n",
    "\n",
    "history_null = gt.mcmc_anneal(state_nullelayer, niter=10,beta_range=(1, 10), mcmc_equilibrate_args=dict(force_niter=iterations),history=True,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Non-layer-corrected DL:\\t\", state_nullelayer.entropy())\n",
    "print(\"Layer-corrected DL:\\t\", state.entropy())\n",
    "print(u\"ln \\u039b: \", state.entropy()-state_nullelayer.entropy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy = pd.DataFrame(np.zeros(shape=(1,2)),columns=['Layered Model','Null Model'])\n",
    "entropy.iloc[0,1] = state_nullelayer.entropy()\n",
    "entropy.iloc[0,0] = state.entropy()\n",
    "entropy=pd.melt(entropy).rename(columns={\"variable\": \"Model\", \"value\": \"Entropy\"})\n",
    "entropy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state.get_levels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 1))\n",
    "sns.barplot(data=entropy,y='Model',x='Entropy').set(ylabel='')\n",
    "# plt.xticks([0.0*1e6,0.2*1e6,0.4*1e6,0.6*1e6,0.8*1e6,1.0*1e6,1.2*1e6],rotation=20,horizontalalignment=\"right\")\n",
    "plt.savefig(out+'model_entropy.png', dpi=600,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(state_nullelayer,open(out+'state_nullelayer_thresh.p','wb'))\n",
    "pickle.dump(state,open(out+'state_layered_thresh.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = load_graph(out+\"graph_file_4mm_space_layered_thresh.xml.gz\")\n",
    "g\n",
    "\n",
    "# g.save(path+\"graph_file_4mm_space_layered_thresh.xml.gz\")\n",
    "# print(g)\n",
    "\n",
    "infile = open(out+'state_nullelayer_thresh.p','rb')\n",
    "state_nullelayer=pickle.load(infile)\n",
    "\n",
    "infile = open(out+'state_layered_thresh.p','rb')\n",
    "state=pickle.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_equilibration(model):\n",
    "    print(\"Beginning MCMC equilibration on \"+str(model))\n",
    "    S1 = model.entropy()\n",
    "#     model = model.copy(bs=model.get_bs() + [np.zeros(1)] * 2,sampling = True)\n",
    "    \n",
    "    def collect_num_groups(s):\n",
    "        for l, sl in enumerate(s.get_levels()):\n",
    "            B = sl.get_nonempty_B()\n",
    "            h[l][B] += 1\n",
    "        \n",
    "    h = [np.zeros(g.num_vertices() + 1) for s in model.get_levels()]\n",
    "    \n",
    "    a=mcmc_equilibrate(model,multiflip=True,verbose=True,force_niter=iterations,\n",
    "                    callback=collect_num_groups,history=True,mcmc_args=dict(niter=10))\n",
    "    \n",
    "    i=np.array(a).shape[0]\n",
    "    print('MCMC equilibrated in', i, 'iterations')\n",
    "    \n",
    "    S2 = model.entropy()\n",
    "    print(\"Improvement:\", S2 - S1)\n",
    "    \n",
    "    column_names = [\"nattempts\",\"nmoves\",\"entropy\"]\n",
    "    df = pd.DataFrame(a,columns=column_names)\n",
    "    \n",
    "    return df,h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "state_nullelayer_mcmc=state_nullelayer.copy()\n",
    "state_nullelayer_mcmc_df,state_nullelayer_mcmc_h=model_equilibration(state_nullelayer_mcmc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "state_mcmc=state.copy()\n",
    "state_mcmc_df,state_mcmc_h=model_equilibration(state_mcmc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_nullelayer_mcmc_df['Model']='Null Model'\n",
    "state_mcmc_df['Model']='Layered Model'\n",
    "\n",
    "# entropy = state_mcmc_df.merge(state_nullelayer_mcmc_df, how='inner')\n",
    "entropy=pd.concat([state_mcmc_df, state_nullelayer_mcmc_df], sort=False)\n",
    "entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.set_edge_filter(None)\n",
    "g.set_vertex_filter(None)\n",
    "state.g.set_edge_filter(None)\n",
    "state.g.set_vertex_filter(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l0_prop = state.g.new_vertex_property(\"int\")\n",
    "l0_prop.a = state.project_partition(0,0).get_array()+1\n",
    "state.g.vertex_properties[\"l0\"]=l0_prop\n",
    "\n",
    "l1_prop = state.g.new_vertex_property(\"int\")\n",
    "l1_prop.a = state.project_partition(1,0).get_array()+1\n",
    "# g.vertex_properties[\"l1\"]=l1_prop\n",
    "state.g.vertex_properties[\"l1\"]=l1_prop\n",
    "\n",
    "l2_prop = state.g.new_vertex_property(\"int\")\n",
    "l2_prop.a = state.project_partition(2,0).get_array()+1\n",
    "# g.vertex_properties[\"l2\"]=l2_prop\n",
    "state.g.vertex_properties[\"l2\"]=l2_prop\n",
    "\n",
    "l3_prop = state.g.new_vertex_property(\"int\")\n",
    "l3_prop.a = state.project_partition(3,0).get_array()+1\n",
    "# g.vertex_properties[\"l3\"]=l3_prop\n",
    "state.g.vertex_properties[\"l3\"]=l3_prop\n",
    "\n",
    "l4_prop = state.g.new_vertex_property(\"int\")\n",
    "l4_prop.a = state.project_partition(4,0).get_array()+1\n",
    "# g.vertex_properties[\"l4\"]=l4_prop\n",
    "state.g.vertex_properties[\"l4\"]=l4_prop\n",
    "\n",
    "l5_prop = state.g.new_vertex_property(\"int\")\n",
    "l5_prop.a = state.project_partition(5,0).get_array()+1\n",
    "# g.vertex_properties[\"l5\"]=l5_prop\n",
    "state.g.vertex_properties[\"l5\"]=l5_prop\n",
    "\n",
    "l6_prop = state.g.new_vertex_property(\"int\")\n",
    "l6_prop.a = state.project_partition(6,0).get_array()+1\n",
    "# g.vertex_properties[\"l6\"]=l6_prop\n",
    "state.g.vertex_properties[\"l6\"]=l6_prop\n",
    "\n",
    "l7_prop = state.g.new_vertex_property(\"int\")\n",
    "l7_prop.a = state.project_partition(7,0).get_array()+1\n",
    "# g.vertex_properties[\"l7\"]=l7_prop\n",
    "state.g.vertex_properties[\"l7\"]=l7_prop\n",
    "\n",
    "l8_prop = state.g.new_vertex_property(\"int\")\n",
    "l8_prop.a = state.project_partition(8,0).get_array()+1\n",
    "# g.vertex_properties[\"l8\"]=l8_prop\n",
    "state.g.vertex_properties[\"l8\"]=l8_prop\n",
    "\n",
    "l9_prop = state.g.new_vertex_property(\"int\")\n",
    "l9_prop.a = state.project_partition(9,0).get_array()+1\n",
    "# g.vertex_properties[\"l9\"]=l9_prop\n",
    "state.g.vertex_properties[\"l9\"]=l9_prop\n",
    "\n",
    "# g.save(out+\"graph_file_4mm_space_layered_thresh.xml.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ee_eigen,x_eigen = gt.eigenvector(state.g,weight=gt.prop_to_size(state.g.ep.target_error))\n",
    "# g.vertex_properties[\"target_error_x_eigen\"]=x_eigen\n",
    "state.g.vp[\"target_error_x_eigen\"]=x_eigen\n",
    "\n",
    "#betweenness centrality needs to be rescaled as it cannot take negative weights, as done so here...\n",
    "#vp_between, ep_between = gt.betweenness(g)\n",
    "vp_between, ep_between = gt.betweenness(state.g,weight=state.g.ep.target_error)\n",
    "# g.vertex_properties[\"target_error_vp_between\"]=vp_between\n",
    "state.g.vertex_properties[\"target_error_vp_between\"]=vp_between\n",
    "\n",
    "#closeness centrality needs to be rescaled as it cannot take negative weights, as done so here...\n",
    "c = gt.closeness(state.g,weight=state.g.ep.target_error)\n",
    "# g.vertex_properties[\"target_error_c\"]=c\n",
    "state.g.vertex_properties[\"target_error_c\"]=c\n",
    "\n",
    "ee_hits, authority_hits, hub_hits = gt.hits(state.g,weight=state.g.ep.target_error)\n",
    "# g.vertex_properties[\"target_error_authority_hits\"]=authority_hits\n",
    "# g.vertex_properties[\"target_error_hub_hits\"]=hub_hits\n",
    "state.g.vertex_properties[\"target_error_authority_hits\"]=authority_hits\n",
    "state.g.vertex_properties[\"target_error_hub_hits\"]=hub_hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ee_eigen,x_eigen = gt.eigenvector(state.g,weight=gt.prop_to_size(state.g.ep.lw))\n",
    "# g.vertex_properties[\"lw_x_eigen\"]=x_eigen\n",
    "state.g.vertex_properties[\"lw_x_eigen\"]=x_eigen\n",
    "\n",
    "#betweenness centrality needs to be rescaled as it cannot take negative weights, as done so here...\n",
    "#vp_between, ep_between = gt.betweenness(g)\n",
    "vp_between, ep_between = gt.betweenness(state.g,weight=state.g.ep.lw)\n",
    "# g.vertex_properties[\"lw_vp_between\"]=vp_between\n",
    "state.g.vertex_properties[\"lw_vp_between\"]=vp_between\n",
    "\n",
    "#closeness centrality needs to be rescaled as it cannot take negative weights, as done so here...\n",
    "c = gt.closeness(state.g,weight=state.g.ep.lw)\n",
    "# g.vertex_properties[\"lw_c\"]=c\n",
    "state.g.vertex_properties[\"lw_c\"]=c\n",
    "\n",
    "ee_hits, authority_hits, hub_hits = gt.hits(state.g,weight=state.g.ep.lw)\n",
    "# g.vertex_properties[\"lw_authority_hits\"]=authority_hits\n",
    "# g.vertex_properties[\"lw_hub_hits\"]=hub_hits\n",
    "state.g.vertex_properties[\"lw_authority_hits\"]=authority_hits\n",
    "state.g.vertex_properties[\"lw_hub_hits\"]=hub_hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g.save(out+\"graph_file_4mm_space_layered_thresh.xml.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centrality = template_nodes.copy()\n",
    "centrality['target_error_x_eigen']=state.g.vp.target_error_x_eigen.get_array()\n",
    "centrality['target_error_vp_between']=np.log(state.g.vp.target_error_vp_between.get_array())\n",
    "centrality['target_error_c']=state.g.vp.target_error_c.get_array()\n",
    "centrality['target_error_authority_hits']=np.log(state.g.vp.target_error_authority_hits.get_array())\n",
    "centrality['target_error_hub_hits']=np.log(state.g.vp.target_error_hub_hits.get_array())\n",
    "\n",
    "centrality['lw_x_eigen']=state.g.vp.lw_x_eigen.get_array()\n",
    "centrality['lw_vp_between']=np.log(state.g.vp.lw_vp_between.get_array())\n",
    "centrality['lw_c']=state.g.vp.lw_c.get_array()\n",
    "centrality['lw_authority_hits']=np.log(state.g.vp.lw_authority_hits.get_array())\n",
    "centrality['lw_hub_hits']=np.log(state.g.vp.lw_hub_hits.get_array())\n",
    "\n",
    "centrality['l0']=state.g.vp.l0.get_array()\n",
    "centrality['l1']=state.g.vp.l1.get_array()\n",
    "centrality['l2']=state.g.vp.l2.get_array()\n",
    "centrality['l3']=state.g.vp.l3.get_array()\n",
    "centrality['l4']=state.g.vp.l4.get_array()\n",
    "centrality['l5']=state.g.vp.l5.get_array()\n",
    "centrality['l6']=state.g.vp.l6.get_array()\n",
    "centrality['l7']=state.g.vp.l7.get_array()\n",
    "centrality['l8']=state.g.vp.l8.get_array()\n",
    "centrality['l9']=state.g.vp.l9.get_array()\n",
    "\n",
    "centrality.reset_index(drop=True,inplace=True)\n",
    "\n",
    "centrality.replace([-np.inf], np.nan, inplace=True)\n",
    "\n",
    "centrality['target_error_x_eigen'].fillna(value=centrality['target_error_x_eigen'].min(),inplace=True)\n",
    "centrality['target_error_vp_between'].fillna(value=centrality['target_error_vp_between'].min(),inplace=True)\n",
    "centrality['target_error_c'].fillna(value=centrality['target_error_c'].min(),inplace=True)\n",
    "centrality['target_error_authority_hits'].fillna(value=centrality['target_error_authority_hits'].min(),inplace=True)\n",
    "centrality['target_error_hub_hits'].fillna(value=centrality['target_error_hub_hits'].min(),inplace=True)\n",
    "\n",
    "centrality['lw_x_eigen'].fillna(value=centrality['lw_x_eigen'].min(),inplace=True)\n",
    "centrality['lw_vp_between'].fillna(value=centrality['lw_vp_between'].min(),inplace=True)\n",
    "centrality['lw_c'].fillna(value=centrality['lw_c'].min(),inplace=True)\n",
    "centrality['lw_authority_hits'].fillna(value=centrality['lw_authority_hits'].min(),inplace=True)\n",
    "centrality['lw_hub_hits'].fillna(value=centrality['lw_hub_hits'].min(),inplace=True)\n",
    "\n",
    "centrality[['target_error_x_eigen','target_error_vp_between','target_error_c','target_error_authority_hits','target_error_hub_hits',\n",
    "            'lw_x_eigen','lw_vp_between','lw_c','lw_authority_hits','lw_hub_hits']]+=0.1\n",
    "\n",
    "centrality.to_csv(out+'centrality_metrics.csv')\n",
    "\n",
    "centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_mask = nib.load('/media/Data1/NewIQ/jruffle_analysis/mni_masks_444/empty_444.nii.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_data = np.zeros(shape=empty_mask.dataobj.shape)\n",
    "\n",
    "for i, row in centrality.iterrows():\n",
    "#     print(i)\n",
    "    x,y,z = (centrality.loc[i,'x']-1),centrality.loc[i,'y'],centrality.loc[i,'z']\n",
    "    z-=1\n",
    "    y-=1\n",
    "    value = centrality.iloc[i,25]\n",
    "    array_data[x,y,z]=value\n",
    "    \n",
    "array_image=nib.Nifti1Image(array_data,empty_mask.affine)\n",
    "nib.save(array_image,out+'layered_target_error_eigen.nii.gz')\n",
    "\n",
    "\n",
    "array_data = np.zeros(shape=empty_mask.dataobj.shape)\n",
    "\n",
    "for i, row in centrality.iterrows():\n",
    "#     print(i)\n",
    "    x,y,z = (centrality.loc[i,'x']-1),centrality.loc[i,'y'],centrality.loc[i,'z']\n",
    "    z-=1\n",
    "    y-=1\n",
    "    value = centrality.iloc[i,26]\n",
    "    array_data[x,y,z]=value\n",
    "    \n",
    "array_image=nib.Nifti1Image(array_data,empty_mask.affine)\n",
    "nib.save(array_image,out+'layered_target_error_between.nii.gz')\n",
    "\n",
    "\n",
    "array_data = np.zeros(shape=empty_mask.dataobj.shape)\n",
    "\n",
    "for i, row in centrality.iterrows():\n",
    "#     print(i)\n",
    "    x,y,z = (centrality.loc[i,'x']-1),centrality.loc[i,'y'],centrality.loc[i,'z']\n",
    "    z-=1\n",
    "    y-=1\n",
    "    value = centrality.iloc[i,27]\n",
    "    array_data[x,y,z]=value\n",
    "    \n",
    "array_image=nib.Nifti1Image(array_data,empty_mask.affine)\n",
    "nib.save(array_image,out+'layered_target_error_closeness.nii.gz')\n",
    "\n",
    "\n",
    "array_data = np.zeros(shape=empty_mask.dataobj.shape)\n",
    "\n",
    "for i, row in centrality.iterrows():\n",
    "#     print(i)\n",
    "    x,y,z = (centrality.loc[i,'x']-1),centrality.loc[i,'y'],centrality.loc[i,'z']\n",
    "    z-=1\n",
    "    y-=1\n",
    "    value = centrality.iloc[i,28]\n",
    "    array_data[x,y,z]=value\n",
    "    \n",
    "array_image=nib.Nifti1Image(array_data,empty_mask.affine)\n",
    "nib.save(array_image,out+'layered_target_error_authority.nii.gz')\n",
    "\n",
    "\n",
    "array_data = np.zeros(shape=empty_mask.dataobj.shape)\n",
    "\n",
    "for i, row in centrality.iterrows():\n",
    "#     print(i)\n",
    "    x,y,z = (centrality.loc[i,'x']-1),centrality.loc[i,'y'],centrality.loc[i,'z']\n",
    "    z-=1\n",
    "    y-=1\n",
    "    value = centrality.iloc[i,29]\n",
    "    array_data[x,y,z]=value\n",
    "    \n",
    "array_image=nib.Nifti1Image(array_data,empty_mask.affine)\n",
    "nib.save(array_image,out+'layered_target_error_hub.nii.gz')\n",
    "\n",
    "array_data = np.zeros(shape=empty_mask.dataobj.shape)\n",
    "\n",
    "for i, row in centrality.iterrows():\n",
    "#     print(i)\n",
    "    x,y,z = (centrality.loc[i,'x']-1),centrality.loc[i,'y'],centrality.loc[i,'z']\n",
    "    z-=1\n",
    "    y-=1\n",
    "    value = centrality.iloc[i,30]\n",
    "    array_data[x,y,z]=value\n",
    "    \n",
    "array_image=nib.Nifti1Image(array_data,empty_mask.affine)\n",
    "nib.save(array_image,out+'layered_lw_eigen.nii.gz')\n",
    "\n",
    "\n",
    "array_data = np.zeros(shape=empty_mask.dataobj.shape)\n",
    "\n",
    "for i, row in centrality.iterrows():\n",
    "#     print(i)\n",
    "    x,y,z = (centrality.loc[i,'x']-1),centrality.loc[i,'y'],centrality.loc[i,'z']\n",
    "    z-=1\n",
    "    y-=1\n",
    "    value = centrality.iloc[i,31]\n",
    "    array_data[x,y,z]=value\n",
    "    \n",
    "array_image=nib.Nifti1Image(array_data,empty_mask.affine)\n",
    "nib.save(array_image,out+'layered_lw_between.nii.gz')\n",
    "\n",
    "\n",
    "array_data = np.zeros(shape=empty_mask.dataobj.shape)\n",
    "\n",
    "for i, row in centrality.iterrows():\n",
    "#     print(i)\n",
    "    x,y,z = (centrality.loc[i,'x']-1),centrality.loc[i,'y'],centrality.loc[i,'z']\n",
    "    z-=1\n",
    "    y-=1\n",
    "    value = centrality.iloc[i,32]\n",
    "    array_data[x,y,z]=value\n",
    "    \n",
    "array_image=nib.Nifti1Image(array_data,empty_mask.affine)\n",
    "nib.save(array_image,out+'layered_lw_closeness.nii.gz')\n",
    "\n",
    "\n",
    "array_data = np.zeros(shape=empty_mask.dataobj.shape)\n",
    "\n",
    "for i, row in centrality.iterrows():\n",
    "#     print(i)\n",
    "    x,y,z = (centrality.loc[i,'x']-1),centrality.loc[i,'y'],centrality.loc[i,'z']\n",
    "    z-=1\n",
    "    y-=1\n",
    "    value = centrality.iloc[i,33]\n",
    "    array_data[x,y,z]=value\n",
    "    \n",
    "array_image=nib.Nifti1Image(array_data,empty_mask.affine)\n",
    "nib.save(array_image,out+'layered_lw_authority.nii.gz')\n",
    "\n",
    "\n",
    "array_data = np.zeros(shape=empty_mask.dataobj.shape)\n",
    "\n",
    "for i, row in centrality.iterrows():\n",
    "#     print(i)\n",
    "    x,y,z = (centrality.loc[i,'x']-1),centrality.loc[i,'y'],centrality.loc[i,'z']\n",
    "    z-=1\n",
    "    y-=1\n",
    "    value = centrality.iloc[i,34]\n",
    "    array_data[x,y,z]=value\n",
    "    \n",
    "array_image=nib.Nifti1Image(array_data,empty_mask.affine)\n",
    "nib.save(array_image,out+'layered_lw_hub.nii.gz')\n",
    "\n",
    "\n",
    "array_data = np.zeros(shape=empty_mask.dataobj.shape)\n",
    "\n",
    "for i, row in centrality.iterrows():\n",
    "#     print(i)\n",
    "    x,y,z = (centrality.loc[i,'x']-1),centrality.loc[i,'y'],centrality.loc[i,'z']\n",
    "    z-=1\n",
    "    y-=1\n",
    "    value = centrality.iloc[i,35]\n",
    "    array_data[x,y,z]=value\n",
    "    \n",
    "array_image=nib.Nifti1Image(array_data,empty_mask.affine)\n",
    "nib.save(array_image,out+'layered_sbm_l0.nii.gz')\n",
    "\n",
    "\n",
    "array_data = np.zeros(shape=empty_mask.dataobj.shape)\n",
    "\n",
    "for i, row in centrality.iterrows():\n",
    "#     print(i)\n",
    "    x,y,z = (centrality.loc[i,'x']-1),centrality.loc[i,'y'],centrality.loc[i,'z']\n",
    "    z-=1\n",
    "    y-=1\n",
    "    value = centrality.iloc[i,36]\n",
    "    array_data[x,y,z]=value\n",
    "    \n",
    "array_image=nib.Nifti1Image(array_data,empty_mask.affine)\n",
    "nib.save(array_image,out+'layered_sbm_l1.nii.gz')\n",
    "\n",
    "array_data = np.zeros(shape=empty_mask.dataobj.shape)\n",
    "\n",
    "for i, row in centrality.iterrows():\n",
    "#     print(i)\n",
    "    x,y,z = (centrality.loc[i,'x']-1),centrality.loc[i,'y'],centrality.loc[i,'z']\n",
    "    z-=1\n",
    "    y-=1\n",
    "    value = centrality.iloc[i,37]\n",
    "    array_data[x,y,z]=value\n",
    "    \n",
    "array_image=nib.Nifti1Image(array_data,empty_mask.affine)\n",
    "nib.save(array_image,out+'layered_sbm_l2.nii.gz')\n",
    "\n",
    "\n",
    "array_data = np.zeros(shape=empty_mask.dataobj.shape)\n",
    "\n",
    "for i, row in centrality.iterrows():\n",
    "#     print(i)\n",
    "    x,y,z = (centrality.loc[i,'x']-1),centrality.loc[i,'y'],centrality.loc[i,'z']\n",
    "    z-=1\n",
    "    y-=1\n",
    "    value = centrality.iloc[i,38]\n",
    "    array_data[x,y,z]=value\n",
    "    \n",
    "array_image=nib.Nifti1Image(array_data,empty_mask.affine)\n",
    "nib.save(array_image,out+'layered_sbm_l3.nii.gz')\n",
    "\n",
    "\n",
    "array_data = np.zeros(shape=empty_mask.dataobj.shape)\n",
    "\n",
    "for i, row in centrality.iterrows():\n",
    "#     print(i)\n",
    "    x,y,z = (centrality.loc[i,'x']-1),centrality.loc[i,'y'],centrality.loc[i,'z']\n",
    "    z-=1\n",
    "    y-=1\n",
    "    value = centrality.iloc[i,39]\n",
    "    array_data[x,y,z]=value\n",
    "    \n",
    "array_image=nib.Nifti1Image(array_data,empty_mask.affine)\n",
    "nib.save(array_image,out+'layered_sbm_l4.nii.gz')\n",
    "\n",
    "\n",
    "array_data = np.zeros(shape=empty_mask.dataobj.shape)\n",
    "\n",
    "for i, row in centrality.iterrows():\n",
    "#     print(i)\n",
    "    x,y,z = (centrality.loc[i,'x']-1),centrality.loc[i,'y'],centrality.loc[i,'z']\n",
    "    z-=1\n",
    "    y-=1\n",
    "    value = centrality.iloc[i,40]\n",
    "    array_data[x,y,z]=value\n",
    "    \n",
    "array_image=nib.Nifti1Image(array_data,empty_mask.affine)\n",
    "nib.save(array_image,out+'layered_sbm_l5.nii.gz')\n",
    "\n",
    "array_data = np.zeros(shape=empty_mask.dataobj.shape)\n",
    "\n",
    "for i, row in centrality.iterrows():\n",
    "#     print(i)\n",
    "    x,y,z = (centrality.loc[i,'x']-1),centrality.loc[i,'y'],centrality.loc[i,'z']\n",
    "    z-=1\n",
    "    y-=1\n",
    "    value = centrality.iloc[i,41]\n",
    "    array_data[x,y,z]=value\n",
    "    \n",
    "array_image=nib.Nifti1Image(array_data,empty_mask.affine)\n",
    "nib.save(array_image,out+'layered_sbm_l6.nii.gz')\n",
    "\n",
    "for i, row in centrality.iterrows():\n",
    "#     print(i)\n",
    "    x,y,z = (centrality.loc[i,'x']-1),centrality.loc[i,'y'],centrality.loc[i,'z']\n",
    "    z-=1\n",
    "    y-=1\n",
    "    value = centrality.iloc[i,42]\n",
    "    array_data[x,y,z]=value\n",
    "    \n",
    "array_image=nib.Nifti1Image(array_data,empty_mask.affine)\n",
    "nib.save(array_image,out+'layered_sbm_l7.nii.gz')\n",
    "\n",
    "\n",
    "for i, row in centrality.iterrows():\n",
    "#     print(i)\n",
    "    x,y,z = (centrality.loc[i,'x']-1),centrality.loc[i,'y'],centrality.loc[i,'z']\n",
    "    z-=1\n",
    "    y-=1\n",
    "    value = centrality.iloc[i,43]\n",
    "    array_data[x,y,z]=value\n",
    "    \n",
    "array_image=nib.Nifti1Image(array_data,empty_mask.affine)\n",
    "nib.save(array_image,out+'layered_sbm_l8.nii.gz')\n",
    "\n",
    "\n",
    "for i, row in centrality.iterrows():\n",
    "#     print(i)\n",
    "    x,y,z = (centrality.loc[i,'x']-1),centrality.loc[i,'y'],centrality.loc[i,'z']\n",
    "    z-=1\n",
    "    y-=1\n",
    "    value = centrality.iloc[i,44]\n",
    "    array_data[x,y,z]=value\n",
    "    \n",
    "array_image=nib.Nifti1Image(array_data,empty_mask.affine)\n",
    "nib.save(array_image,out+'layered_sbm_l9.nii.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "state.g.set_edge_filter(None)\n",
    "\n",
    "range(9)\n",
    "levels = [3]\n",
    "\n",
    "# 1/np.exp(model_comparison_prediction['target_error'])\n",
    "\n",
    "def mean_confidence_interval(data, confidence=0.95):\n",
    "    a = 1.0 * np.array(data)\n",
    "    n = len(a)\n",
    "    m, se = np.mean(a), scipy.stats.sem(a)\n",
    "    h = se * scipy.stats.t.ppf((1 + confidence) / 2., n-1)\n",
    "    return m, m-h, m+h\n",
    "\n",
    "rowsnum = 0\n",
    "\n",
    "for l in range(9):\n",
    "    block = state.project_partition(l,0)\n",
    "    rowsnum += len(np.unique(np.array(block.get_array())))\n",
    "    \n",
    "rowsnum\n",
    "\n",
    "parcels = pd.DataFrame(np.zeros(shape=(rowsnum,9)),columns=['Level','Block label','Target Error Posterior Mean',\n",
    "                                                            'Target error 95% CI lower','Target error 95% CI upper',\n",
    "                                                            'Lesion Weight Posterior Mean','Lesion Weight 95% CI lower',\n",
    "                                                            'Lesion Weight 95% CI upper',\"Significant\"])\n",
    "parcels\n",
    "\n",
    "row=0\n",
    "\n",
    "for l in range(9):\n",
    "# for l in levels:\n",
    "    print(\"hierarchy\")\n",
    "    print(l)\n",
    "    \n",
    "    block = state.project_partition(l,0)\n",
    "    print(\"blocks...\")\n",
    "    print(block.get_array())\n",
    "    print(\"\")\n",
    "    print(\"unique blocks...\")\n",
    "    print(len(np.unique(np.array(block.get_array()))))\n",
    "    print(\"\")\n",
    "\n",
    "    print(state.g)\n",
    "    \n",
    "    template_path = out+'layered_sbm_l'+str(l)+'.nii.gz'\n",
    "    template = nib.load(template_path)\n",
    "    template.affine\n",
    "    posterior = np.asanyarray(template.dataobj)\n",
    "    sd = np.asanyarray(template.dataobj)\n",
    "    posterior_lw = posterior.copy()\n",
    "    sd_lw = sd.copy()\n",
    "    \n",
    "    counter=1\n",
    "    for b in np.unique(np.array(block.get_array())):\n",
    "        print(\"working on block...\" +str(b)+\": \"+str(counter)+\"/\"+str(len(np.unique(np.array(block.get_array()))))) \n",
    "        \n",
    "        block_filter = state.g.new_vertex_property(\"bool\") \n",
    "        for v in state.g.vertices(): \n",
    "                if block[v] ==b:\n",
    "                    block_filter[v] = True \n",
    "                else: \n",
    "                    block_filter[v] = False \n",
    "\n",
    "        g_block = state.g.copy()\n",
    "        g_block.set_vertex_filter(block_filter)\n",
    "        g_block.purge_vertices()\n",
    "        g_block.purge_edges()\n",
    "\n",
    "#         block_weights = g_block.ep.target_error.get_array() / g_block.ep.lw.get_array()\n",
    "        block_weights = np.array(g_block.ep.target_error.get_array())\n",
    "        block_weights_m = np.mean(block_weights)\n",
    "#         print(block_weights_m)\n",
    "        block_weights_std = numpy.std(block_weights)\n",
    "        \n",
    "        block_weights_lw = np.array(g_block.ep.lw.get_array())\n",
    "        block_weights_lw_m = np.mean(block_weights_lw)\n",
    "#         print(block_weights_lw_m)\n",
    "        block_weights_lw_std = numpy.std(block_weights_lw)\n",
    "    \n",
    "        b+=1 ###because we had to increment by 1 for the niftis\n",
    "        posterior[posterior==b] = block_weights_m\n",
    "        sd[sd==b] = block_weights_std\n",
    "        \n",
    "        posterior_lw[posterior_lw==b] = block_weights_lw_m\n",
    "        sd_lw[sd_lw==b] = block_weights_lw_std        \n",
    "        \n",
    "        parcels.iloc[row,0]=l\n",
    "        parcels.iloc[row,1]=b\n",
    "        parcels.iloc[row,2:5] = mean_confidence_interval(block_weights)\n",
    "        parcels.iloc[row,5:8] = mean_confidence_interval(block_weights_lw)\n",
    "        \n",
    "        a1, a2 = parcels.iloc[row,3:5].values\n",
    "        b1, b2 = parcels.iloc[row,6:8].values\n",
    "\n",
    "        if max(a2, b2) - min(a1, b1) >= (a2 - a1) + (b2 - b1):\n",
    "            parcels.iloc[row,8]=\"*\"\n",
    "            \n",
    "        if max(a2, b2) - min(a1, b1) < (a2 - a1) + (b2 - b1):\n",
    "            parcels.iloc[row,8]=\"NSD\"\n",
    "        \n",
    "        counter+=1\n",
    "        row+=1\n",
    "        \n",
    "    posteriornifti = nib.Nifti1Image(posterior,template.affine)\n",
    "    nib.save(posteriornifti,out+'layered_sbm_l'+str(l)+'_posterior_mean.nii.gz')\n",
    "\n",
    "    sdnifti = nib.Nifti1Image(sd,template.affine)\n",
    "    nib.save(sdnifti,out+'layered_sbm_l'+str(l)+'_posterior_sd.nii.gz')\n",
    "    \n",
    "    \n",
    "    posteriornifti_lw = nib.Nifti1Image(posterior_lw,template.affine)\n",
    "    nib.save(posteriornifti_lw,out+'layered_sbm_l'+str(l)+'_posterior_mean_lesionweight.nii.gz')\n",
    "\n",
    "    sdnifti_lw = nib.Nifti1Image(sd_lw,template.affine)\n",
    "    nib.save(sdnifti_lw,out+'layered_sbm_l'+str(l)+'_posterior_sd_lesionweight.nii.gz')\n",
    "    \n",
    "    impath = out+'layered_sbm_l'+str(l)+'_posterior_mean.nii.gz'\n",
    "    fig=plotting.plot_glass_brain(impath, colorbar=True,\n",
    "                          plot_abs=False,\n",
    "                         threshold=0)\n",
    "    \n",
    "    impath = out+'layered_sbm_l'+str(l)+'_posterior_mean_lesionweight.nii.gz'\n",
    "    fig=plotting.plot_glass_brain(impath, colorbar=True,\n",
    "                          plot_abs=False,\n",
    "                         threshold=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parcels.dropna(inplace=True)\n",
    "parcels.reset_index(drop=True,inplace=True)\n",
    "parcels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "sns.histplot(parcels['Target Error Posterior Mean'],bins=50)\n",
    "plt.xlabel(task+' Posterior Mean')\n",
    "plt.savefig(out+'block_posterior_means_target_error.png', dpi=600,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "sns.histplot(parcels['Lesion Weight Posterior Mean'],bins=50)\n",
    "plt.savefig(out+'block_posterior_means_target_error.png', dpi=600,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temp=parcels.copy()\n",
    "temp['CI Direction']=temp['Significant']\n",
    "temp\n",
    "\n",
    "for i, row in temp.iterrows():\n",
    "    print(i)\n",
    "    if temp.iloc[i,2] > temp.iloc[i,5]:\n",
    "        print(\"target_error\")\n",
    "        temp.iloc[i,9] = 'Target Error Dominant Block'\n",
    "    if temp.iloc[i,2] < temp.iloc[i,5]:\n",
    "        print(\"LESION\")\n",
    "        temp.iloc[i,9] = 'Lesion Dominant Block'\n",
    "        \n",
    "    if temp.iloc[i,8] =='NSD':\n",
    "        temp.iloc[i,9] = 'NSD'\n",
    "        \n",
    "# temp = temp[temp['Level']>0]\n",
    "temp = temp[temp['Level']<6]\n",
    "temp.reset_index(drop=True,inplace=True)\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.loc[temp['CI Direction'] == '*', 'CI Direction']='NSD'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp[temp['CI Direction'] == '*']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp['CI Direction'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "sns.scatterplot(data=temp,x='Target Error Posterior Mean',y='Lesion Weight Posterior Mean',hue='CI Direction',\n",
    "               palette=[\"r\", \"b\", \"k\"]).set(xlim=(None,0.75),ylim=(None,0.75))\n",
    "plt.xlabel(task+' Posterior Mean')\n",
    "plt.savefig(out+'block_posterior_means_scatter.png', dpi=600,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(parcels.shape)\n",
    "parcels.head()\n",
    "parcels.to_csv(out+'parcels_posteriors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(temp.shape)\n",
    "temp.head()\n",
    "temp.to_csv(out+'parcels_posteriors_withsiglabels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parcel_looper = temp[temp['Level']==0].copy()\n",
    "parcel_looper['Difference']=parcel_looper['Target Error Posterior Mean']-parcel_looper['Lesion Weight Posterior Mean']\n",
    "parcel_looper[parcel_looper['Significant']!='NSD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "sns.histplot(parcel_looper['Difference'],bins=50)\n",
    "plt.savefig(out+'block_posterior_means_difference.png', dpi=600,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=3\n",
    "\n",
    "for l in range(6):\n",
    "    print(l)\n",
    "    parcel_looper = temp.copy()\n",
    "    parcel_looper[parcel_looper['Level']==l]\n",
    "    parcel_looper['Difference']=parcel_looper['Target Error Posterior Mean']-parcel_looper['Lesion Weight Posterior Mean']\n",
    "    l_path = out+'layered_sbm_l'+str(l)+'.nii.gz'\n",
    "    l_path = nib.load(l_path)\n",
    "    l_path.affine\n",
    "    limage= np.asanyarray(l_path.dataobj)\n",
    "    # workthese = parcel_looper[parcel_looper['Significant']!='NSD'].copy()\n",
    "    # workthese.reset_index(drop=True,inplace=True)\n",
    "    # workthese\n",
    "\n",
    "    parcel_looper.head()\n",
    "\n",
    "    for i, row in parcel_looper.iterrows():\n",
    "        block=parcel_looper.iloc[i,1]\n",
    "        diff = parcel_looper.iloc[i,10]\n",
    "\n",
    "        if parcel_looper.iloc[i,8]=='*':\n",
    "            limage[limage==block]=diff\n",
    "        if parcel_looper.iloc[i,8]=='NSD':\n",
    "            limage[limage==block]=0\n",
    "\n",
    "    limage[limage>1.5]=0\n",
    "\n",
    "    diffnifti = nib.Nifti1Image(limage,template.affine)\n",
    "    nib.save(diffnifti,out+'layered_sbm_l'+str(l)+'_difference_nifti.nii.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parcel_looper['CI Direction'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parcel_looper[parcel_looper['CI Direction']=='target_error/NART Dominant Block']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parcel_looper = temp[temp['Level']==0].copy()\n",
    "parcel_looper['Difference']=parcel_looper['Target Error Posterior Mean']-parcel_looper['Lesion Weight Posterior Mean']\n",
    "parcel_looper[parcel_looper['Significant']!='NSD']\n",
    "parcel_looper\n",
    "\n",
    "sns.color_palette(\"icefire\", as_cmap=True)\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "sns.scatterplot(data=parcel_looper,x='Target Error Posterior Mean',y='Lesion Weight Posterior Mean',hue='Difference',\n",
    "               palette=\"icefire\").set(xlim=(None,0.75),ylim=(None,0.75))\n",
    "plt.xlabel(task+' Posterior Mean')\n",
    "plt.savefig(out+'block_posterior_means_scatter_hue.png', dpi=600,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parcel_looper = temp[temp['Level']==0].copy()\n",
    "parcel_looper['Difference']=parcel_looper['Target Error Posterior Mean']-parcel_looper['Lesion Weight Posterior Mean']\n",
    "parcel_looper[parcel_looper['Significant']!='NSD']\n",
    "parcel_looper['Absolute Difference']=abs(parcel_looper['Difference'])\n",
    "\n",
    "sns.color_palette(\"icefire\", as_cmap=True)\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "sns.scatterplot(data=parcel_looper,x='Target Error Posterior Mean',y='Lesion Weight Posterior Mean',hue='CI Direction',\n",
    "               size='Absolute Difference').set(xlim=(None,0.75),ylim=(None,0.75))\n",
    "plt.xlabel(task+' Posterior Mean')\n",
    "plt.savefig(out+'block_posterior_means_scatter.png', dpi=600,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "sns.scatterplot(data=parcel_looper,x='Target Error Posterior Mean',y='Lesion Weight Posterior Mean',hue='Difference',\n",
    "               size='Absolute Difference',palette=\"icefire\",\n",
    "               legend = False).set(xlim=(None,0.4),ylim=(None,0.7))\n",
    "plt.xlabel(task+' Posterior Mean')\n",
    "plt.savefig(out+'block_posterior_means_scatter_hue.png', dpi=600,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state.g.set_edge_filter(None)\n",
    "thresh = np.percentile(state.g.ep.target_error.get_array(), 75)\n",
    "\n",
    "target_error_edge_filter = g.new_edge_property(\"bool\") \n",
    "\n",
    "for e in g.edges(): \n",
    "        if g.ep.target_error[e] >=thresh:\n",
    "            target_error_edge_filter[e] = True \n",
    "        else: \n",
    "            target_error_edge_filter[e] = False \n",
    "# g.edge_properties[\"target_error_thresh_filter\"]=edge_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state.g.set_edge_filter(None)\n",
    "thresh = np.percentile(state.g.ep.lw.get_array(), 75)\n",
    "\n",
    "lw_edge_filter = g.new_edge_property(\"bool\") \n",
    "\n",
    "for e in g.edges(): \n",
    "        if g.ep.lw[e] >=thresh:\n",
    "            lw_edge_filter[e] = True \n",
    "        else: \n",
    "            lw_edge_filter[e] = False \n",
    "# g.edge_properties[\"target_error_thresh_filter\"]=edge_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state.g.set_edge_filter(None)\n",
    "print(state.g)\n",
    "state.g.set_edge_filter(target_error_edge_filter)\n",
    "print(state.g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block1 = state.project_partition(0,0)\n",
    "bg, bb, vcount, ecount, avp, aep = \\\n",
    "gt.condensation_graph(state.g,block1 ,self_loops=True)\n",
    "vcount.get_array()\n",
    "state.g.set_edge_filter(None)\n",
    "state.g.set_edge_filter(state.g.ep.target_errorfilter)\n",
    "print(state.g)\n",
    "\n",
    "state.draw(edge_color=gt.prop_to_size(state.g.ep.target_error, power=1, log=True), ecmap=(matplotlib.cm.inferno, .6),\n",
    "#            eorder=g.ep.target_error, edge_pen_width=gt.prop_to_size(g.ep.target_error, 1, 4, power=1, log=True),\n",
    "           edge_gradient=[],\n",
    "          hvertex_size=15,hedge_pen_width=8,\n",
    "#            vertex_size=gt.prop_to_size(state.g.vp.target_error_x_eigen,1,200,power=1),\n",
    "#            vertex_fill_color=gt.prop_to_size(state.g.vp.target_error_x_eigen,power=1),\n",
    "#             vorder=state.g.vp.target_error_x_eigen,\n",
    "           vcmap=(matplotlib.cm.inferno, .6),\n",
    "          output=out+'layered_sbm_target_error.png',output_size=(4000,4000))\n",
    "\n",
    "state.draw(edge_color=gt.prop_to_size(state.g.ep.target_error, power=1, log=True), ecmap=(matplotlib.cm.inferno, .6),\n",
    "#            eorder=g.ep.target_error, edge_pen_width=gt.prop_to_size(g.ep.target_error, 1, 4, power=1, log=True),\n",
    "           edge_gradient=[],\n",
    "          hvertex_size=15,hedge_pen_width=8,\n",
    "#            vertex_size=gt.prop_to_size(state.g.vp.target_error_x_eigen,1,200,power=1),\n",
    "#            vertex_fill_color=gt.prop_to_size(state.g.vp.target_error_x_eigen,power=1),\n",
    "#             vorder=state.g.vp.target_error_x_eigen,\n",
    "           vcmap=(matplotlib.cm.inferno, .6),\n",
    "          output=out+'layered_sbm_target_error.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block1 = state.project_partition(0,0)\n",
    "bg, bb, vcount, ecount, avp, aep = \\\n",
    "gt.condensation_graph(state.g,block1 ,self_loops=True)\n",
    "vcount.get_array()\n",
    "state.g.set_edge_filter(None)\n",
    "state.g.set_edge_filter(state.g.ep.lesionfilter)\n",
    "print(state.g)\n",
    "\n",
    "state.draw(edge_color=gt.prop_to_size(state.g.ep.lw, power=3, log=True), ecmap=(matplotlib.cm.inferno, .9),\n",
    "           eorder=state.g.ep.lw, edge_pen_width=gt.prop_to_size(state.g.ep.lw, 1, 4, power=1, log=True),\n",
    "           edge_gradient=[],\n",
    "          hvertex_size=15,hedge_pen_width=8,\n",
    "#            vertex_size=gt.prop_to_size(state.g.vp.lw_x_eigen,1,200,power=1),\n",
    "#            vertex_fill_color=gt.prop_to_size(state.g.vp.lw_x_eigen,power=1),\n",
    "#             vorder=state.g.vp.lw_x_eigen,\n",
    "           vcmap=(matplotlib.cm.inferno, .6),\n",
    "          output=out+'layered_sbm_lw.png',output_size=(4000,4000))\n",
    "\n",
    "state.draw(edge_color=gt.prop_to_size(state.g.ep.lw, power=3, log=True), ecmap=(matplotlib.cm.inferno, .9),\n",
    "           eorder=state.g.ep.lw, edge_pen_width=gt.prop_to_size(state.g.ep.lw, 1, 4, power=1, log=True),\n",
    "           edge_gradient=[],\n",
    "          hvertex_size=15,hedge_pen_width=8,\n",
    "#            vertex_size=gt.prop_to_size(state.g.vp.lw_x_eigen,1,200,power=1),\n",
    "#            vertex_fill_color=gt.prop_to_size(state.g.vp.lw_x_eigen,power=1),\n",
    "#             vorder=state.g.vp.lw_x_eigen,\n",
    "           vcmap=(matplotlib.cm.inferno, .6),\n",
    "          output=out+'layered_sbm_lw.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state.g.set_edge_filter(None)\n",
    "state.g.set_vertex_filter(None)\n",
    "\n",
    "t = gt.get_hierarchy_tree(state)[0]\n",
    "# tpos = pos = gt.radial_tree_layout(t, t.vertex(t.num_vertices() - 1), weighted=True)\n",
    "tpos = pos = gt.radial_tree_layout(t, t.vertex(0), weighted=True)\n",
    "cts = gt.get_hierarchy_control_points(g, t, tpos)\n",
    "pos = g.own_property(tpos)\n",
    "b = state.levels[0].b\n",
    "shape = b.copy()\n",
    "shape.a %= len(np.unique(b.get_array()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt.draw_hierarchy(state,pos=pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# block1 = state.project_partition(0,0)\n",
    "# bg, bb, vcount, ecount, avp, aep = \\\n",
    "# gt.condensation_graph(state.g,block1 ,self_loops=True)\n",
    "# vcount.get_array()\n",
    "state.g.set_edge_filter(None)\n",
    "state.g.set_edge_filter(state.g.ep.target_errorfilter)\n",
    "print(state.g)\n",
    "\n",
    "state.draw(pos=pos,edge_color=gt.prop_to_size(state.g.ep.target_error, power=1, log=True), ecmap=(matplotlib.cm.inferno, .6),\n",
    "           eorder=state.g.ep.target_error, edge_pen_width=gt.prop_to_size(state.g.ep.target_error, 1, 4, power=1, log=True),\n",
    "           edge_gradient=[],\n",
    "          hvertex_size=15,hedge_pen_width=8,\n",
    "           vertex_size=gt.prop_to_size(state.g.vp.target_error_x_eigen,1,200,power=1),\n",
    "           vertex_fill_color=gt.prop_to_size(state.g.vp.target_error_x_eigen,power=1),\n",
    "            vorder=state.g.vp.target_error_x_eigen,\n",
    "           vcmap=(matplotlib.cm.inferno, .6),\n",
    "          output=out+'layered_sbm_target_error.png',output_size=(4000,4000))\n",
    "\n",
    "state.draw(pos=pos,edge_color=gt.prop_to_size(state.g.ep.target_error, power=1, log=True), ecmap=(matplotlib.cm.inferno, .6),\n",
    "           eorder=state.g.ep.target_error, edge_pen_width=gt.prop_to_size(state.g.ep.target_error, 1, 4, power=1, log=True),\n",
    "           edge_gradient=[],\n",
    "          hvertex_size=15,hedge_pen_width=8,\n",
    "           vertex_size=gt.prop_to_size(state.g.vp.target_error_x_eigen,1,200,power=1),\n",
    "           vertex_fill_color=gt.prop_to_size(state.g.vp.target_error_x_eigen,power=1),\n",
    "            vorder=state.g.vp.target_error_x_eigen,\n",
    "           vcmap=(matplotlib.cm.inferno, .6),\n",
    "          output=out+'layered_sbm_target_error.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_nullelayer.g.set_edge_filter(None)\n",
    "state_nullelayer.g.set_edge_filter(state_nullelayer.g.ep.target_errorfilter)\n",
    "print(state_nullelayer.g)\n",
    "\n",
    "state_nullelayer.draw(edge_color=gt.prop_to_size(state_nullelayer.g.ep.target_error, power=1, log=True), ecmap=(matplotlib.cm.inferno, .6),\n",
    "           eorder=state_nullelayer.g.ep.target_error, edge_pen_width=gt.prop_to_size(state_nullelayer.g.ep.target_error, 1, 4, power=1, log=True),\n",
    "           edge_gradient=[],\n",
    "          hvertex_size=15,hedge_pen_width=8,\n",
    "          output=out+'null_layered_sbm_target_error.png',output_size=(4000,4000))\n",
    "\n",
    "state_nullelayer.draw(edge_color=gt.prop_to_size(state_nullelayer.g.ep.target_error, power=1, log=True), ecmap=(matplotlib.cm.inferno, .6),\n",
    "           eorder=state_nullelayer.g.ep.target_error, edge_pen_width=gt.prop_to_size(state_nullelayer.g.ep.target_error, 1, 4, power=1, log=True),\n",
    "           edge_gradient=[],\n",
    "          hvertex_size=15,hedge_pen_width=8,\n",
    "          output=out+'null_layered_sbm_target_error.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# block1 = state.project_partition(0,0)\n",
    "# bg, bb, vcount, ecount, avp, aep = \\\n",
    "# gt.condensation_graph(state.g,block1 ,self_loops=True)\n",
    "# vcount.get_array()\n",
    "state.g.set_edge_filter(None)\n",
    "state.g.set_edge_filter(state.g.ep.target_errorfilter)\n",
    "print(state.g)\n",
    "\n",
    "state.draw(\n",
    "    pos=pos,\n",
    "           edge_color=gt.prop_to_size(state.g.ep.target_error, power=1, log=True), ecmap=(matplotlib.cm.inferno, .6),\n",
    "           eorder=state.g.ep.target_error, edge_pen_width=gt.prop_to_size(state.g.ep.target_error, 1, 4, power=1, log=True),\n",
    "           edge_gradient=[],\n",
    "          hvertex_size=15,hedge_pen_width=8,\n",
    "           vertex_size=gt.prop_to_size(state.g.vp.target_error_x_eigen,1,200,power=1),\n",
    "           vertex_fill_color=gt.prop_to_size(state.g.vp.target_error_x_eigen,power=1),\n",
    "            vorder=state.g.vp.target_error_x_eigen,\n",
    "           vcmap=(matplotlib.cm.inferno, .6),\n",
    "#     hide=3,\n",
    "          output=out+'layered_sbm_target_error_nohedge.png',output_size=(4000,4000))\n",
    "\n",
    "state.draw(\n",
    "    pos=pos,\n",
    "           edge_color=gt.prop_to_size(state.g.ep.target_error, power=1, log=True), ecmap=(matplotlib.cm.inferno, .6),\n",
    "           eorder=state.g.ep.target_error, edge_pen_width=gt.prop_to_size(state.g.ep.target_error, 1, 4, power=1, log=True),\n",
    "           edge_gradient=[],\n",
    "          hvertex_size=15,hedge_pen_width=8,\n",
    "           vertex_size=gt.prop_to_size(state.g.vp.target_error_x_eigen,1,200,power=1),\n",
    "           vertex_fill_color=gt.prop_to_size(state.g.vp.target_error_x_eigen,power=1),\n",
    "            vorder=state.g.vp.target_error_x_eigen,\n",
    "           vcmap=(matplotlib.cm.inferno, .6),\n",
    "#     hide=3,\n",
    "          output=out+'layered_sbm_target_error_nohedge.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# block1 = state.project_partition(0,0)\n",
    "# bg, bb, vcount, ecount, avp, aep = \\\n",
    "# gt.condensation_graph(state.g,block1 ,self_loops=True)\n",
    "# vcount.get_array()\n",
    "state.g.set_edge_filter(None)\n",
    "state.g.set_edge_filter(state.g.ep.lesionfilter)\n",
    "print(state.g)\n",
    "\n",
    "state.draw(pos=pos,edge_color=gt.prop_to_size(state.g.ep.lw, power=3, log=True), ecmap=(matplotlib.cm.inferno, .9),\n",
    "           eorder=state.g.ep.lw, edge_pen_width=gt.prop_to_size(state.g.ep.lw, 1, 4, power=1, log=True),\n",
    "           edge_gradient=[],\n",
    "          hvertex_size=15,hedge_pen_width=8,\n",
    "           vertex_size=gt.prop_to_size(state.g.vp.lw_x_eigen,1,200,power=1),\n",
    "           vertex_fill_color=gt.prop_to_size(state.g.vp.lw_x_eigen,power=1),\n",
    "            vorder=state.g.vp.lw_x_eigen,\n",
    "           vcmap=(matplotlib.cm.inferno, .6),\n",
    "          output=out+'layered_sbm_lw.png',output_size=(4000,4000))\n",
    "\n",
    "state.draw(pos=pos,edge_color=gt.prop_to_size(state.g.ep.lw, power=3, log=True), ecmap=(matplotlib.cm.inferno, .9),\n",
    "           eorder=state.g.ep.lw, edge_pen_width=gt.prop_to_size(state.g.ep.lw, 1, 4, power=1, log=True),\n",
    "           edge_gradient=[],\n",
    "          hvertex_size=15,hedge_pen_width=8,\n",
    "           vertex_size=gt.prop_to_size(state.g.vp.lw_x_eigen,1,200,power=1),\n",
    "           vertex_fill_color=gt.prop_to_size(state.g.vp.lw_x_eigen,power=1),\n",
    "            vorder=state.g.vp.lw_x_eigen,\n",
    "           vcmap=(matplotlib.cm.inferno, .6),\n",
    "          output=out+'layered_sbm_lw.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state.g.set_edge_filter(None)\n",
    "state.g.set_edge_filter(state.g.ep.lesionfilter)\n",
    "print(state.g)\n",
    "\n",
    "state.draw(pos=pos,edge_color=gt.prop_to_size(state.g.ep.lw, power=5, log=True), ecmap=(matplotlib.cm.inferno, .9),\n",
    "           eorder=state.g.ep.lw, edge_pen_width=gt.prop_to_size(state.g.ep.lw, 1, 4, power=1, log=True),\n",
    "           edge_gradient=[],\n",
    "          hvertex_size=15,hedge_pen_width=8,\n",
    "           vertex_size=gt.prop_to_size(state.g.vp.lw_x_eigen,1,200,power=1),\n",
    "           vertex_fill_color=gt.prop_to_size(state.g.vp.lw_x_eigen,power=1),\n",
    "            vorder=state.g.vp.lw_x_eigen,\n",
    "           vcmap=(matplotlib.cm.inferno, .6),\n",
    "#            hide=20,\n",
    "          output=out+'layered_sbm_lw_nohedge.png',output_size=(4000,4000))\n",
    "\n",
    "state.draw(pos=pos,edge_color=gt.prop_to_size(state.g.ep.lw, power=5, log=True), ecmap=(matplotlib.cm.inferno, .9),\n",
    "           eorder=state.g.ep.lw, edge_pen_width=gt.prop_to_size(state.g.ep.lw, 1, 4, power=1, log=True),\n",
    "           edge_gradient=[],\n",
    "          hvertex_size=15,hedge_pen_width=8,\n",
    "           vertex_size=gt.prop_to_size(state.g.vp.lw_x_eigen,1,200,power=1),\n",
    "           vertex_fill_color=gt.prop_to_size(state.g.vp.lw_x_eigen,power=1),\n",
    "            vorder=state.g.vp.lw_x_eigen,\n",
    "           vcmap=(matplotlib.cm.inferno, .6),\n",
    "#            hide=20,\n",
    "          output=out+'layered_sbm_lw_nohedge.svg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
