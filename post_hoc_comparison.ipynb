{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stroop, fluency, and the ACC post hoc tests\n",
    "# J Ruffle j.ruffle@ucl.ac.uk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph_tool.all import *; import graph_tool.all as gt; import  matplotlib; import math; import numpy; import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import matplotlib\n",
    "#matplotlib.use('Agg')\n",
    "import pandas as pd\n",
    "import scipy as scipy\n",
    "import seaborn as sns\n",
    "from pylab import *\n",
    "from multiprocessing import  Pool\n",
    "import subprocess\n",
    "import scipy.sparse as sparse\n",
    "from scipy.io import savemat\n",
    "# sns.set_theme()\n",
    "import pickle\n",
    "pd.set_option('display.max_columns', 500)\n",
    "import glob\n",
    "\n",
    "import nibabel as nib\n",
    "\n",
    "from nilearn import plotting\n",
    "from nilearn import surface\n",
    "from nilearn import datasets\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "from tqdm.notebook import tqdm \n",
    "\n",
    "custom_params = {\"axes.spines.right\": False, \"axes.spines.top\": False}\n",
    "sns.set_theme(style=\"ticks\", rc=custom_params)\n",
    "\n",
    "sns.set_palette('Set2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/jruffle/OneDrive/PhD/Stroop_GLDM/11_CORTEX_PAPER_analysis/'\n",
    "inpath = path+'inputs/'\n",
    "outpath = path+'results/'\n",
    "\n",
    "level = 1\n",
    "debug=True\n",
    "invert_stroop=True\n",
    "pca=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open(inpath+'state_layered_thresh.p','rb')\n",
    "state=pickle.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parcellation_nii = nib.load(inpath+'layered_sbm_l'+str(level)+'.nii.gz')\n",
    "parcellation = np.asanyarray(parcellation_nii.dataobj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_roi(parcellation_nii, title=\"Stochastic block model parcellation: L\"+str(level))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_error_nii = nib.load(inpath+'layered_sbm_l'+str(level)+'_posterior_mean.nii.gz')\n",
    "plotting.plot_glass_brain(target_error_nii, title='Stroop reconstruction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(inpath+'Stroop_3_05_23.xlsx')\n",
    "target = 'Stroop_words_correct'\n",
    "\n",
    "df['Hosp_Number']=''\n",
    "for i, row in df.iterrows():\n",
    "    df.loc[i,'Hosp_Number']=row['Original_scan_filename'].split('_')[0].split('.')[0]\n",
    "df.drop(['Group','Laterality','Original_scan_filename','Lesion_mask_filename'],axis=1,inplace=True)\n",
    "\n",
    "#add column for each parcel at level\n",
    "for p in np.unique(parcellation[parcellation>0]):\n",
    "    df[int(p)]=0\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clamp(train):\n",
    "    perc01 = np.percentile(train, 5, keepdims=True)\n",
    "    perc99 = np.percentile(train, 95, keepdims=True)\n",
    "    clamped_train = np.clip(train, a_min=perc01, a_max=perc99)\n",
    "    return clamped_train\n",
    "\n",
    "def zscore(train):\n",
    "    m = np.mean(train)\n",
    "    sd = np.std(train)l\n",
    "    zscored_train = (train-m)/sd\n",
    "    return zscored_train\n",
    "\n",
    "def normalise(train):\n",
    "    normalised_train = (train - np.min(train)) / (np.max(train) - np.min(train))\n",
    "    return normalised_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Stroop_words_correct'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if invert_stroop:\n",
    "    df['Stroop_words_correct']=1/df['Stroop_words_correct']\n",
    "    if debug:\n",
    "        print('Inverting stroop target') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['Stroop_words_correct'] = normalise(zscore(clamp(df['Stroop_words_correct'].values)))\n",
    "        \n",
    "sns.histplot(df['Stroop_words_correct'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in df.iterrows():\n",
    "    print(row['Stroop_words_correct'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stroop_nii = nib.load(inpath+'stroop_layered_sbm_l0_posterior_mean_clusterthreshold.nii.gz')\n",
    "affine = stroop_nii.affine\n",
    "stroop = np.asanyarray(stroop_nii.dataobj)\n",
    "stroop[stroop<0.35]=0\n",
    "stroop[stroop>0]=1\n",
    "stroop=stroop.astype(int)\n",
    "\n",
    "cingulate_nii = nib.load(inpath+'stroop_444_thresh_cc_bin.nii.gz')\n",
    "cingulate = np.asanyarray(cingulate_nii.dataobj)\n",
    "cingulate=cingulate.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parcellation_stroop = parcellation.copy()\n",
    "parcellation_cingulate = parcellation.copy()\n",
    "\n",
    "parcellation_stroop = parcellation_stroop*stroop\n",
    "parcellation_cingulate = parcellation_cingulate*cingulate\n",
    "\n",
    "stroop_parcels = np.unique(parcellation_stroop[parcellation_stroop>0]).astype(int)\n",
    "cingulate_parcels = np.unique(parcellation_cingulate[parcellation_cingulate>0]).astype(int)\n",
    "\n",
    "if debug:\n",
    "    print('Stroop parcels: '+str(stroop_parcels))\n",
    "    print(\"\")\n",
    "    print('Cingulate parcels: '+str(cingulate_parcels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parcellation_cingulate_nii = nib.Nifti1Image(parcellation_cingulate,affine=affine)\n",
    "parcellation_stroop_nii = nib.Nifti1Image(parcellation_stroop,affine=affine)\n",
    "\n",
    "nib.save(parcellation_cingulate_nii,outpath+'parcellation_cingulate_nii.nii.gz')\n",
    "nib.save(parcellation_stroop_nii,outpath+'parcellation_stroop_nii.nii.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_roi(parcellation_cingulate_nii, title=\"Cingulate internal parcellation: L\"+str(level))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_roi(parcellation_stroop_nii, title=\"Stroop internal parcellation: L\"+str(level))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_these = []\n",
    "df_volume = df.copy()\n",
    "df_prop = df.copy()\n",
    "worked_counter =0\n",
    "\n",
    "counts_native = np.unique(parcellation,return_counts=True)\n",
    "counts_native = pd.DataFrame(counts_native).T\n",
    "counts_native.rename(columns={0:'parcel',1:'Volume'},inplace=True)\n",
    "counts_native\n",
    "\n",
    "lesion_path = '/home/jruffle/OneDrive/PhD/Stroop_GLDM/11_CORTEX_PAPER_analysis/inputs/mni_masks_444/'\n",
    "for participant, row in tqdm(df.iterrows(),total=len(df)):\n",
    "    \n",
    "    sub = row['Hosp_Number']\n",
    "    lesion = glob.glob(lesion_path+'*'+sub+'*')\n",
    "    \n",
    "    if len(lesion)==0:\n",
    "        drop_these.append(sub)\n",
    "        df.drop(participant,axis=0,inplace=True)\n",
    "        df_volume.drop(participant,axis=0,inplace=True)\n",
    "        df_prop.drop(participant,axis=0,inplace=True)\n",
    "        \n",
    "        if debug:\n",
    "            print('Appending to drop list: '+str(sub))\n",
    "        \n",
    "    if len(lesion)>0:\n",
    "        segmentation_nii = nib.load(lesion[0])\n",
    "        segmentation = np.asanyarray(segmentation_nii.dataobj)\n",
    "        segmentation[segmentation>0]=1\n",
    "        \n",
    "        parcellation_segmentation = parcellation.copy()\n",
    "        parcellation_segmentation = parcellation_segmentation*segmentation\n",
    "        \n",
    "        if debug:\n",
    "            print(sub)\n",
    "            print('Parcels in lesion: '+str(np.unique(parcellation_segmentation[parcellation_segmentation>0])))\n",
    "        worked_counter +=1\n",
    "            \n",
    "        df.loc[participant,np.unique(parcellation_segmentation[parcellation_segmentation>0]).astype(int)]=1\n",
    "        \n",
    "        counts = np.unique(parcellation_segmentation,return_counts=True)\n",
    "        \n",
    "        for count in range(len(counts[0])):\n",
    "            if counts[0][count]>0:\n",
    "                df_volume.loc[participant,int(counts[0][count])]=int(counts[1][count])\n",
    "                \n",
    "                #prop\n",
    "                total_vol = counts_native.loc[counts_native['parcel']==counts[0][count],'Volume'].values[0]\n",
    "                prop = int(counts[1][count]) / total_vol\n",
    "                df_prop.loc[participant,int(counts[0][count])]=prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['Age','Hosp_Number'],axis=1,inplace=True)\n",
    "df.reset_index(drop=True,inplace=True)\n",
    "df_volume.drop(['Age','Hosp_Number'],axis=1,inplace=True)\n",
    "df_volume.reset_index(drop=True,inplace=True)\n",
    "df_prop.drop(['Age','Hosp_Number'],axis=1,inplace=True)\n",
    "df_prop.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_cols(dataframe,pca=pca,mode='l_frontal'):\n",
    "    for c in dataframe.columns:\n",
    "        if c !='Stroop_words_correct':\n",
    "            if pca:\n",
    "                dataframe.rename(columns={c:mode+'_PC_'+str(c)},inplace=True)\n",
    "            else:\n",
    "                dataframe.rename(columns={c:'parcel_'+str(c)},inplace=True)\n",
    "    dataframe.reset_index(drop=True,inplace=True)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pca:\n",
    "    from sklearn.decomposition import PCA\n",
    "    print('Enabling PCA mode')\n",
    "    comps=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pca(df,comp=5):\n",
    "    pca = PCA(n_components=comp)\n",
    "    pca_df = pca.fit_transform(df)\n",
    "    print(pca.fit(df).explained_variance_ratio_.sum())\n",
    "    return pca_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_parcels = np.concatenate((stroop_parcels,cingulate_parcels))\n",
    "\n",
    "Stroop_words_correct = df['Stroop_words_correct']\n",
    "\n",
    "df = df[all_parcels]\n",
    "df['Stroop_words_correct']=Stroop_words_correct\n",
    "\n",
    "df_prop = df_prop[all_parcels]\n",
    "df_prop['Stroop_words_correct']=Stroop_words_correct\n",
    "\n",
    "df_volume = df_volume[all_parcels]\n",
    "df_volume['Stroop_words_correct']=Stroop_words_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stroop = df[stroop_parcels]\n",
    "if pca:\n",
    "    df_stroop = pd.DataFrame(normalise(generate_pca(df[stroop_parcels])))\n",
    "\n",
    "df_stroop['Stroop_words_correct']=Stroop_words_correct\n",
    "df_stroop = rename_cols(df_stroop)\n",
    "df_stroop.to_csv(outpath+'df_stroop.csv')\n",
    "\n",
    "df_cingulate = df[cingulate_parcels]\n",
    "if pca:\n",
    "    df_cingulate = pd.DataFrame(normalise(generate_pca(df[cingulate_parcels])))\n",
    "df_cingulate['Stroop_words_correct']=Stroop_words_correct\n",
    "df_cingulate = rename_cols(df_cingulate,mode='cingulate')\n",
    "df_cingulate.to_csv(outpath+'df_cingulate.csv')\n",
    "\n",
    "if pca:\n",
    "    df = pd.concat([df_stroop,df_cingulate],axis=1)\n",
    "    df = df.loc[:,~df.columns.duplicated()].copy()\n",
    "if not pca:\n",
    "    df = rename_cols(df)\n",
    "df.to_csv(outpath+'df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prop_stroop = df_prop[stroop_parcels]\n",
    "if pca:\n",
    "    df_prop_stroop = pd.DataFrame(normalise(generate_pca(df_prop[stroop_parcels])))\n",
    "df_prop_stroop['Stroop_words_correct']=Stroop_words_correct\n",
    "df_prop_stroop = rename_cols(df_prop_stroop)\n",
    "df_prop_stroop.to_csv(outpath+'df_prop_stroop.csv')\n",
    "\n",
    "df_prop_cingulate = df_prop[cingulate_parcels]\n",
    "if pca:\n",
    "    df_prop_cingulate = pd.DataFrame(normalise(generate_pca(df_prop[cingulate_parcels])))\n",
    "df_prop_cingulate['Stroop_words_correct']=Stroop_words_correct\n",
    "df_prop_cingulate = rename_cols(df_prop_cingulate,mode='cingulate')\n",
    "df_prop_cingulate.to_csv(outpath+'df_prop_cingulate.csv')\n",
    "\n",
    "if pca:\n",
    "    df_prop = pd.concat([df_prop_stroop,df_prop_cingulate],axis=1)\n",
    "    df_prop = df_prop.loc[:,~df_prop.columns.duplicated()].copy()\n",
    "if not pca:\n",
    "    df_prop = rename_cols(df_prop)\n",
    "df_prop.to_csv(outpath+'df_prop.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler\n",
    "# scale = StandardScaler()\n",
    "from sklearn import preprocessing\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_volume_stroop = df_volume[stroop_parcels]\n",
    "if pca:\n",
    "    df_volume_stroop = pd.DataFrame(normalise(generate_pca(df_volume[stroop_parcels])))\n",
    "df_volume_stroop['Stroop_words_correct']=Stroop_words_correct\n",
    "df_volume_stroop = rename_cols(df_volume_stroop)\n",
    "df_volume_stroop.to_csv(outpath+'df_volume_stroop.csv')\n",
    "\n",
    "df_volume_cingulate = df_volume[cingulate_parcels]\n",
    "if pca:\n",
    "    df_volume_cingulate = pd.DataFrame(normalise(generate_pca(df_volume[cingulate_parcels])))\n",
    "df_volume_cingulate['Stroop_words_correct']=Stroop_words_correct\n",
    "df_volume_cingulate = rename_cols(df_volume_cingulate,mode='cingulate')\n",
    "df_volume_cingulate.to_csv(outpath+'df_volume_cingulate.csv')\n",
    "\n",
    "if pca:\n",
    "    df_volume = pd.concat([df_volume_stroop,df_volume_cingulate],axis=1)\n",
    "    df_volume = df_volume.loc[:,~df_volume.columns.duplicated()].copy()\n",
    "if not pca:\n",
    "    df_volume = rename_cols(df_volume)\n",
    "df_volume.to_csv(outpath+'df_volume.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,3,figsize=(20,5))\n",
    "\n",
    "sns.heatmap(df_volume.corr(),ax=ax[0])\n",
    "ax[0].set_title('Volumetric corrmat')\n",
    "\n",
    "sns.heatmap(df.corr(),ax=ax[1])\n",
    "ax[1].set_title('Binary corrmat')\n",
    "\n",
    "sns.heatmap(df_prop.corr(),ax=ax[2])\n",
    "ax[2].set_title('Proportionate corrmat')\n",
    "\n",
    "plt.savefig(outpath+\"bayesreg_corr\",dpi=300,bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "try:\n",
    "    os.remove(outpath+'bayes_reg_posteriors.txt')\n",
    "    print('removing last attempt')\n",
    "except:\n",
    "    print('no file to remove')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Running matlab script now')\n",
    "!matlab -nodisplay -nosplash -nodesktop -r \"run('/home/jruffle/OneDrive/PhD/scripts_JKR/fluid_intelligence/newIQ-graph-lesion-deficit-mapping/stroop_bayesreg.m');exit;\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_reg = pd.read_csv(outpath+'bayes_reg.csv')\n",
    "priors = ['ridge','g','hs','hs+','lasso']\n",
    "\n",
    "bayes_reg['Damage formulation'] = ''\n",
    "bayes_reg['Parcels']=''\n",
    "\n",
    "for i, row in bayes_reg.iterrows():\n",
    "    if 'cin' in row['model']:\n",
    "        bayes_reg.loc[i,'Parcels']='Cingulate'\n",
    "    if 'str' in row['model']:\n",
    "        bayes_reg.loc[i,'Parcels']='Left frontal'\n",
    "    if 'all' in row['model']:\n",
    "        bayes_reg.loc[i,'Parcels']='All'\n",
    "\n",
    "    if 'bin' in row['model']:\n",
    "        bayes_reg.loc[i,'Damage formulation']='Binary'\n",
    "    if 'vol' in row['model']:\n",
    "        bayes_reg.loc[i,'Damage formulation']='Volumetric'\n",
    "    if 'pro' in row['model']:\n",
    "        bayes_reg.loc[i,'Damage formulation']='Proportionate'\n",
    "\n",
    "bayes_reg.replace('df_cin_bin','Cingulate parcel (NeuroQuery) (binary impact)',inplace=True)\n",
    "bayes_reg.replace('df_str_bin','Left frontal stroop parcel (ours) (binary impact)',inplace=True)\n",
    "bayes_reg.replace('df_all_bin','All parcels (binary impact)',inplace=True)\n",
    "bayes_reg.replace('df_cin_vol','Cingulate parcel (NeuroQuery) (volumetric impact)',inplace=True)\n",
    "bayes_reg.replace('df_str_vol','Left frontal stroop parcel (ours) (volumetric impact)',inplace=True)\n",
    "bayes_reg.replace('df_all_vol','All parcels (volumetric impact)',inplace=True)\n",
    "bayes_reg.replace('df_cin_pro','Cingulate parcel (NeuroQuery) (proportionate impact)',inplace=True)\n",
    "bayes_reg.replace('df_str_pro','Left frontal stroop parcel (ours) (proportionate impact)',inplace=True)\n",
    "bayes_reg.replace('df_all_pro','All parcels (proportionate impact)',inplace=True)\n",
    "\n",
    "bayes_reg.to_csv(outpath+'bayes_reg_revised_labels.csv')\n",
    "bayes_reg.sort_values(by='r2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_reg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2,figsize=(10,5))\n",
    "sns.boxplot(data=bayes_reg,x='Parcels',y='r2',hue='Damage formulation',ax=ax[0],legend=False)\n",
    "# ax[0].set_title('Bayesian Regression R2')\n",
    "ax[0].set(ylabel='R2')\n",
    "ax[0].set_xticklabels(ax[0].get_xticklabels(), rotation=15,ha='right',rotation_mode='anchor')\n",
    "\n",
    "sns.boxplot(data=bayes_reg,x='Parcels',y='waic',hue='Damage formulation',ax=ax[1])\n",
    "# ax[1].set_title('Bayesian Regression WAIC')\n",
    "ax[1].set(ylabel='WAIC')\n",
    "ax[1].set_xticklabels(ax[1].get_xticklabels(), rotation=15,ha='right',rotation_mode='anchor')\n",
    "\n",
    "fig.suptitle(\"Bayesian Regression\",y=0.93)\n",
    "\n",
    "plt.savefig(outpath+\"bayesreg_subplot\",dpi=300,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2,figsize=(10,5))\n",
    "sns.boxplot(data=bayes_reg,hue='Parcels',y='r2',x='Damage formulation',ax=ax[0],legend=False)\n",
    "# ax[0].set_title('Bayesian Regression R2')\n",
    "ax[0].set(ylabel='R2')\n",
    "ax[0].set_xticklabels(ax[0].get_xticklabels(), rotation=15,ha='right',rotation_mode='anchor')\n",
    "\n",
    "sns.boxplot(data=bayes_reg,hue='Parcels',y='waic',x='Damage formulation',ax=ax[1])\n",
    "# ax[1].set_title('Bayesian Regression WAIC')\n",
    "ax[1].set(ylabel='WAIC')\n",
    "ax[1].set_xticklabels(ax[1].get_xticklabels(), rotation=15,ha='right',rotation_mode='anchor')\n",
    "\n",
    "fig.suptitle(\"Bayesian Regression\",y=0.93)\n",
    "\n",
    "plt.savefig(outpath+\"bayesreg_subplot_recoloured\",dpi=300,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filename = '/home/jruffle/OneDrive/PhD/Stroop_GLDM/11_CORTEX_PAPER_analysis/results/bayes_reg_posteriors.txt'\n",
    "# # filename = \"./dataset/sample_data.txt\"\n",
    "# coef = pd.read_csv(filename)\n",
    "\n",
    "file = open(filename, \"r\")\n",
    "content = file.read()\n",
    "print(content)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filename, 'r') as file:\n",
    "    lines = file.readlines()[11:-3]\n",
    "column_headers = ['Parameter','mean(Coef)','std(Coef)','95% Cred ll','95% Cred ul','tStat','Rank','ESS']\n",
    "asterisks = []\n",
    "\n",
    "coef_df = pd.DataFrame(columns=column_headers)\n",
    "for c in range(len(lines)):\n",
    "    split_line = lines[c].split(' ')\n",
    "    split_line = list(filter(None, split_line))\n",
    "    try:\n",
    "        split_line.remove('|')\n",
    "    except:\n",
    "        print('not done')\n",
    "    matching = [s for s in split_line if \"*\" in s]\n",
    "    coef_df.loc[c] = split_line[0],split_line[1],split_line[2],split_line[3],split_line[4],split_line[5],split_line[6],split_line[-1][:-3]\n",
    "    if len(matching)>0:\n",
    "        asterisks.append(matching[0])\n",
    "    if len(matching)==0:\n",
    "        asterisks.append('')\n",
    "coef_df['Significance']=asterisks\n",
    "coef_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if pca:\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parcellation_nii = nib.load(inpath+'layered_sbm_l'+str(level)+'.nii.gz')\n",
    "parcellation_bayes_reg = parcellation.copy()\n",
    "parcels_to_loop = np.unique(parcellation[parcellation>0]).astype(int)\n",
    "sig_thresh = ['*','**']\n",
    "# sig_thresh = ['**']\n",
    "\n",
    "for p in parcels_to_loop:\n",
    "    if p in parcellation_bayes_reg:\n",
    "        subset = coef_df.loc[coef_df['Parameter']=='parcel_'+str(p)]\n",
    "        \n",
    "        if len(subset['Significance'])>0:\n",
    "        \n",
    "            if subset['Significance'].values[0] in sig_thresh:\n",
    "                if debug:\n",
    "                    print('Found something significant: '+str(p))\n",
    "                parcellation_bayes_reg[parcellation_bayes_reg==p]=float(subset['mean(Coef)'].values[0])\n",
    "                \n",
    "            else:\n",
    "                if debug:\n",
    "                    print('zero: '+str(p))\n",
    "                parcellation_bayes_reg[parcellation_bayes_reg==p]=0\n",
    "        else:\n",
    "            if debug:\n",
    "                print('zero: '+str(p))\n",
    "            parcellation_bayes_reg[parcellation_bayes_reg==p]=0\n",
    "    else:\n",
    "        if debug:\n",
    "            print('zero: '+str(p))\n",
    "        parcellation_bayes_reg[parcellation_bayes_reg==p]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parcellation_bayes_reg_nii = nib.Nifti1Image(parcellation_bayes_reg,affine=affine)\n",
    "nib.save(parcellation_bayes_reg_nii,outpath+'parcellation_bayes_reg_nii.nii.gz')\n",
    "plotting.plot_glass_brain(parcellation_bayes_reg_nii, title='BayesReg Coefficients',colorbar=True,plot_abs=False)\n",
    "plt.savefig(outpath+'bayesreg.png', dpi=600,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parcellation_bayes_reg_pos = parcellation_bayes_reg.copy()\n",
    "parcellation_bayes_reg_pos[parcellation_bayes_reg_pos<0]=0\n",
    "parcellation_bayes_reg_nii_pos = nib.Nifti1Image(parcellation_bayes_reg_pos,affine=affine)\n",
    "nib.save(parcellation_bayes_reg_nii_pos,outpath+'parcellation_bayes_reg_nii_pos.nii.gz')\n",
    "plotting.plot_glass_brain(parcellation_bayes_reg_nii_pos, title='BayesReg Coefficients (Pos)',colorbar=True,plot_abs=False)\n",
    "plt.savefig(outpath+'bayesreg_positive_only.png', dpi=600,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parcellation_bayes_reg_neg = parcellation_bayes_reg.copy()\n",
    "parcellation_bayes_reg_neg[parcellation_bayes_reg_neg>0]=0\n",
    "parcellation_bayes_reg_nii_neg = nib.Nifti1Image(parcellation_bayes_reg_neg,affine=affine)\n",
    "nib.save(parcellation_bayes_reg_nii_neg,outpath+'parcellation_bayes_reg_nii_neg.nii.gz')\n",
    "plotting.plot_glass_brain(parcellation_bayes_reg_nii_neg, title='BayesReg Coefficients (Neg)',colorbar=True,plot_abs=False)\n",
    "plt.savefig(outpath+'bayesreg_negative_only.png', dpi=600,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import surface\n",
    "from nilearn import datasets\n",
    "\n",
    "# stat_img = datasets.load_sample_motor_activation_image()\n",
    "stat_img = outpath+'parcellation_bayes_reg_nii_pos.nii.gz'\n",
    "\n",
    "big_fsaverage = datasets.fetch_surf_fsaverage('fsaverage')\n",
    "big_texture = surface.vol_to_surf(stat_img, big_fsaverage.pial_left)\n",
    "\n",
    "plotting.plot_surf_stat_map(big_fsaverage.infl_left,\n",
    "                            big_texture, hemi='left', colorbar=True,\n",
    "                            title='BayesReg - Regional damage ~ lower Stroop score',\n",
    "                            threshold=0.001, bg_map=big_fsaverage.sulc_left,\n",
    "                           )\n",
    "\n",
    "plt.savefig(outpath+'bayesreg_pos_3d.png', dpi=600,bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_img_on_surf(stat_img,\n",
    "                          views=['lateral', 'medial'],\n",
    "                          hemispheres=['left'],\n",
    "                          colorbar=True,title='BayesReg - Regional damage ~ lower Stroop score',\n",
    "                            threshold=0.001)\n",
    "plotting.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsaverage = datasets.fetch_surf_fsaverage()\n",
    "stat_img = outpath+'parcellation_bayes_reg_nii_pos.nii.gz'\n",
    "\n",
    "texture = surface.vol_to_surf(stat_img, fsaverage.pial_left)\n",
    "\n",
    "destrieux_atlas = datasets.fetch_atlas_surf_destrieux()\n",
    "parcellation = destrieux_atlas['map_left']\n",
    "\n",
    "# these are the regions we want to outline\n",
    "regions_dict = {b'G_postcentral': 'Postcentral gyrus',\n",
    "                b'G_precentral': 'Precentral gyrus'}\n",
    "\n",
    "# get indices in atlas for these labels\n",
    "regions_indices = [\n",
    "    np.where(np.array(destrieux_atlas['labels']) == region)[0][0]\n",
    "    for region in regions_dict\n",
    "]\n",
    "\n",
    "labels = list(regions_dict.values())\n",
    "\n",
    "figure = plotting.plot_surf_stat_map(fsaverage.infl_left,\n",
    "                                     texture, hemi='left',\n",
    "                                     title='BayesReg - Regional damage ~ lower Stroop score',\n",
    "                                     colorbar=True, threshold=.0001,\n",
    "                                     bg_map=fsaverage.sulc_left)\n",
    "\n",
    "plotting.plot_surf_contours(fsaverage.infl_left, parcellation, labels=labels,\n",
    "                            levels=regions_indices, figure=figure,\n",
    "                            legend=True,\n",
    "                            colors=['g', 'k'])\n",
    "\n",
    "plt.savefig(outpath+'bayesreg_pos_3d_with_labels.png', dpi=600,bbox_inches='tight')\n",
    "plotting.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
